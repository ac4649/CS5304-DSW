{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PAD_TOKEN = '_PAD_'\n",
    "UNK_TOKEN = '_UNK_'\n",
    "\n",
    "\n",
    "mydir = os.path.dirname(os.path.abspath('data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/adriencogny/GIT/CS5304-DSW/assignments/assignment4'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "class BagOfWordsModel(nn.Module):\n",
    "  def __init__(self, embeddings):\n",
    "    super(BagOfWordsModel, self).__init__()\n",
    "        \n",
    "    self.embed = nn.Embedding(embeddings.shape[0], embeddings.shape[1], sparse=True)\n",
    "    self.embed.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "    self.classify = nn.Linear(embeddings.shape[1], 5)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.classify(self.embed(x).sum(1))\n",
    "\n",
    "\n",
    "# Train a CNN Sentiment Classifier using pre-trained\n",
    "# and frozen skip-gram word2vec as your word embedding\n",
    "# on the Sentiment data from above\n",
    "class CNNClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, embeddings, output_size, kernel_dim=100, kernel_sizes=(3, 4, 5), dropout=0.5):\n",
    "        super(CNNClassifier,self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab.shape[0], embeddings.shape[0])\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, kernel_dim, (K, embeddings.shape[0])) for K in kernel_sizes])\n",
    "\n",
    "        # kernal_size = (K,D) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * kernel_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def init_weights(self, pretrained_word_vectors, is_static=False):\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(pretrained_word_vectors).float())\n",
    "        if is_static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, inputs, is_training=False):\n",
    "        inputs = self.embedding(inputs).unsqueeze(1) # (B,1,T,D)\n",
    "        inputs = [F.relu(conv(inputs)).squeeze(3) for conv in self.convs] #[(N,Co,W), ...]*len(Ks)\n",
    "        inputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in inputs] #[(N,Co), ...]*len(Ks)\n",
    "\n",
    "        concated = torch.cat(inputs, 1)\n",
    "\n",
    "        if is_training:\n",
    "            concated = self.dropout(concated) # (N,len(Ks)*Co)\n",
    "        out = self.fc(concated) \n",
    "        return F.log_softmax(out,1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenConverter(object):\n",
    "  def __init__(self, vocab):\n",
    "    self.vocab = vocab\n",
    "    self.unknown = 0\n",
    "\n",
    "  def convert(self, token):\n",
    "    if token in self.vocab:\n",
    "      id = self.vocab.get(token)\n",
    "    else:\n",
    "      id = self.vocab.get(UNK_TOKEN)\n",
    "      self.unknown += 1\n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods for loading SST data\n",
    "\n",
    "def sentiment2label(x):\n",
    "  if x >= 0 and x <= 0.2:\n",
    "    return 0\n",
    "  elif x > 0.2 and x <= 0.4:\n",
    "    return 1\n",
    "  elif x > 0.4 and x <= 0.6:\n",
    "    return 2\n",
    "  elif x > 0.6 and x <= 0.8:\n",
    "    return 3\n",
    "  elif x > 0.8 and x <= 1:\n",
    "    return 4\n",
    "  else:\n",
    "    raise ValueError('Improper sentiment value {}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeWord(word,lemmatizer,position='n'):\n",
    "    #this function lemmatizes the word given\n",
    "    \n",
    "    newWord = lemmatizer.lemmatize(word,pos=position)\n",
    "    \n",
    "    if position == 'n':\n",
    "        nextPosition = 'v'\n",
    "    elif position == 'v':\n",
    "        nextPosition = 'a'\n",
    "    elif position == 'a':\n",
    "        nextPosition = 'r'\n",
    "    else:\n",
    "        return newWord\n",
    "    \n",
    "    if newWord == word:\n",
    "        #nothing changed, try something else\n",
    "        newWord = lemmatizeWord(word,lemmatizer,nextPosition)\n",
    "        \n",
    "    \n",
    "    return newWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(wordVector):\n",
    "    noStop = list(filter(lambda x: x not in nltk.corpus.stopwords.words('english'), wordVector))\n",
    "#     print(noStop)\n",
    "    return noStop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will do all of the stemming, lemmatization,\n",
    "# removal of stop words and other things required for the generation of vocab\n",
    "def tokenizePhrase(phrase,removeStop = False, lemmatize = False):\n",
    "    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    \n",
    "    returnTokens = phrase.split(' ')\n",
    "    \n",
    "    if lemmatize:\n",
    "        lematizedTokens = []\n",
    "        for word in phraseWords:\n",
    "            newWord = lemmatizeWord(word,lemmatizer)\n",
    "            lematizedTokens.append(newWord)\n",
    "        returnTokens = lematizedTokens\n",
    "        \n",
    "#     print(returnTokens)\n",
    "    if removeStop:\n",
    "        returnTokens = removeStopWords(returnTokens)\n",
    "        \n",
    "    return returnTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dictionary_txt_with_phrase_ids(dictionary_path, phrase_ids_path, labels_path=None):\n",
    "  print('Reading data dictionary_path={} phrase_ids_path={} labels_path={}'.format(\n",
    "    dictionary_path, phrase_ids_path, labels_path))\n",
    "\n",
    "  with open(phrase_ids_path) as f:\n",
    "    phrase_ids = set(line.strip() for line in f)\n",
    "\n",
    "  with open(dictionary_path) as f:\n",
    "    examples_dict = dict()\n",
    "    for line in f:\n",
    "#       print(line)\n",
    "      parts = line.strip().split('|')\n",
    "      phrase = parts[0]\n",
    "      phrase_id = parts[1]\n",
    "\n",
    "      if phrase_id not in phrase_ids:\n",
    "        continue\n",
    "\n",
    "      example = dict()\n",
    "      example['phrase'] = phrase.replace('(', '-LRB').replace(')', '-RRB-')\n",
    "      example['tokens'] = tokenizePhrase(example['phrase'])\n",
    "      example['example_id'] = phrase_id\n",
    "      example['label'] = None\n",
    "    \n",
    "    \n",
    "#       print(example)\n",
    "    \n",
    "      examples_dict[example['example_id']] = example\n",
    "\n",
    "  if labels_path is not None:\n",
    "    with open(labels_path) as f:\n",
    "      for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "          continue\n",
    "        parts = line.strip().split('|')\n",
    "        phrase_id = parts[0]\n",
    "        sentiment = float(parts[1])\n",
    "        label = sentiment2label(sentiment)\n",
    "\n",
    "        if phrase_id in examples_dict:\n",
    "          examples_dict[phrase_id]['label'] = label\n",
    "\n",
    "  examples = [ex for _, ex in examples_dict.items()]\n",
    "\n",
    "  print('Found {} examples.'.format(len(examples)))\n",
    "\n",
    "  return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(datasets):\n",
    "    vocab = dict()\n",
    "    vocab[PAD_TOKEN] = len(vocab)\n",
    "    vocab[UNK_TOKEN] = len(vocab)\n",
    "    for data in datasets:\n",
    "        for example in data:\n",
    "          for word in example['tokens']:\n",
    "            if word not in vocab:\n",
    "              vocab[word] = len(vocab)\n",
    "\n",
    "    print('Vocab size: {}'.format(len(vocab)))\n",
    "#     pint(vocab)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2ids(data, vocab):\n",
    "  converter = TokenConverter(vocab)\n",
    "  for example in data:\n",
    "    example['tokens'] = list(map(converter.convert, example['tokens']))\n",
    "  print('Found {} unknown tokens.'.format(converter.unknown))\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_embeddings(data_path, phrase_ids_path, embeddings_path):\n",
    "  labels_path = os.path.join(data_path, 'sentiment_labels.txt')\n",
    "  dictionary_path = os.path.join(data_path, 'dictionary.txt')\n",
    "  train_data = read_dictionary_txt_with_phrase_ids(dictionary_path, os.path.join(phrase_ids_path, 'phrase_ids.train.txt'), labels_path)\n",
    "  validation_data = read_dictionary_txt_with_phrase_ids(dictionary_path, os.path.join(phrase_ids_path, 'phrase_ids.dev.txt'), labels_path)\n",
    "  test_data = read_dictionary_txt_with_phrase_ids(dictionary_path, os.path.join(phrase_ids_path, 'phrase_ids.test.txt'))\n",
    "  vocab = build_vocab([train_data, validation_data, test_data])\n",
    "  vocab, embeddings = load_embeddings(options.embeddings, vocab, cache=True)\n",
    "  train_data = convert2ids(train_data, vocab)\n",
    "  validation_data = convert2ids(validation_data, vocab)\n",
    "  test_data = convert2ids(test_data, vocab)\n",
    "  return train_data, validation_data, test_data, vocab, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path, vocab, cache=False, cache_path=None):\n",
    "  print(\"Loading Embeddings\")\n",
    "#   print(vocab)\n",
    "  rows = []\n",
    "  new_vocab = [UNK_TOKEN]\n",
    "\n",
    "  if cache_path is None:\n",
    "    cache_path = path + '.cache'\n",
    "\n",
    "  # Use cache file if it exists.\n",
    "  if os.path.exists(cache_path):\n",
    "    path = cache_path\n",
    "\n",
    "  print(\"Reading embeddings from {}\".format(path))\n",
    "\n",
    "  # first pass over the embeddings to vocab and relevant rows\n",
    "  with open(path) as f:\n",
    "    for line in f:\n",
    "      word, row = line.split(' ', 1)\n",
    "      if word == UNK_TOKEN:\n",
    "        raise ValueError('The unk token should not exist w.in embeddings.')\n",
    "      if word in vocab:\n",
    "        rows.append(line)\n",
    "        new_vocab.append(word)\n",
    "\n",
    "  # optionally save relevant rows to cache file.\n",
    "  if cache and not os.path.exists(cache_path):\n",
    "    with open(cache_path, 'w') as f:\n",
    "      for line in rows:\n",
    "        f.write(line)\n",
    "      print(\"Cached embeddings to {}\".format(cache_path))\n",
    "\n",
    "  # turn vocab list into a dictionary\n",
    "  new_vocab = {w: i for i, w in enumerate(new_vocab)}\n",
    "\n",
    "  print('New vocab size: {}'.format(len(new_vocab)))\n",
    "\n",
    "  assert len(rows) == len(new_vocab) - 1\n",
    "\n",
    "  # create embeddings matrix\n",
    "  embeddings = np.zeros((len(new_vocab), 300), dtype=np.float32)\n",
    "  for i, line in enumerate(rows):\n",
    "    embeddings[i+1] = list(map(float, line.strip().split(' ')[1:]))\n",
    "    \n",
    "#   print(new_vocab)\n",
    "  return new_vocab, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Iterator\n",
    "\n",
    "def prepare_data(data):\n",
    "#     print(\"Preparing data\")\n",
    "    # pad data\n",
    "    maxlen = max(map(len, data))\n",
    "    data = [ex + [0] * (maxlen-len(ex)) for ex in data]\n",
    "#     print(data)\n",
    "\n",
    "    # wrap in tensor\n",
    "    return torch.LongTensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels(labels):\n",
    "  try:\n",
    "    return torch.LongTensor(labels)\n",
    "  except:\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(dataset, batch_size, forever=False):\n",
    "  dataset_size = len(dataset)\n",
    "  order = None\n",
    "  nbatches = dataset_size // batch_size\n",
    "\n",
    "  def init_order():\n",
    "    return random.sample(range(dataset_size), dataset_size)\n",
    "\n",
    "  def get_batch(start, end):\n",
    "    batch = [dataset[ii] for ii in order[start:end]]\n",
    "    data = prepare_data([ex['tokens'] for ex in batch])\n",
    "    labels = prepare_labels([ex['label'] for ex in batch])\n",
    "    example_ids = [ex['example_id'] for ex in batch]\n",
    "    return data, labels, example_ids\n",
    "\n",
    "  order = init_order()\n",
    "\n",
    "  while True:\n",
    "    for i in range(nbatches):\n",
    "      start = i*batch_size\n",
    "      end = (i+1)*batch_size\n",
    "      yield get_batch(start, end)\n",
    "\n",
    "    if nbatches*batch_size < dataset_size:\n",
    "      yield get_batch(nbatches*batch_size, dataset_size)\n",
    "\n",
    "    if not forever:\n",
    "      break\n",
    "    \n",
    "    order = init_order()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Methods\n",
    "\n",
    "def checkpoint_model(step, val_err, model, opt, save_path):\n",
    "  save_dict = dict(\n",
    "    step=step,\n",
    "    val_err=val_err,\n",
    "    model_state_dict=model.state_dict(),\n",
    "    opt_state_dict=opt.state_dict())\n",
    "  torch.save(save_dict, save_path)\n",
    "\n",
    "\n",
    "def load_model(model, opt, load_path):\n",
    "  load_dict = torch.load(load_path)\n",
    "  step = load_dict['step']\n",
    "  val_err = load_dict['val_err']\n",
    "  model.load_state_dict(load_dict['model_state_dict'])\n",
    "  opt.load_state_dict(load_dict['opt_state_dict'])\n",
    "  return step, val_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adriencogny/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/adriencogny/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "<__main__.Namespace object at 0x1a283c8ac8>\n",
      "Reading data dictionary_path=/Users/adriencogny/GIT/CS5304-DSW/assignments/assignment4/data/stanfordSentimentTreebank/dictionary.txt phrase_ids_path=/Users/adriencogny/GIT/CS5304-DSW/assignments/assignment4/phrase_ids.train.txt labels_path=/Users/adriencogny/GIT/CS5304-DSW/assignments/assignment4/data/stanfordSentimentTreebank/sentiment_labels.txt\n",
      "Found 159274 examples.\n",
      "Reading data dictionary_path=/Users/adriencogny/GIT/CS5304-DSW/assignments/assignment4/data/stanfordSentimentTreebank/dictionary.txt phrase_ids_path=/Users/adriencogny/GIT/CS5304-DSW/assignments/assignment4/phrase_ids.dev.txt labels_path=/Users/adriencogny/GIT/CS5304-DSW/assignments/assignment4/data/stanfordSentimentTreebank/sentiment_labels.txt\n",
      "Found 24772 examples.\n",
      "Reading data dictionary_path=/Users/adriencogny/GIT/CS5304-DSW/assignments/assignment4/data/stanfordSentimentTreebank/dictionary.txt phrase_ids_path=/Users/adriencogny/GIT/CS5304-DSW/assignments/assignment4/phrase_ids.test.txt labels_path=None\n",
      "Found 46663 examples.\n",
      "Vocab size: 21703\n",
      "Loading Embeddings\n",
      "Reading embeddings from /Users/adriencogny/GIT/CS5304-DSW/assignments/assignment4/data/glove/glove.840B.300d.txt\n",
      "Cached embeddings to /Users/adriencogny/GIT/CS5304-DSW/assignments/assignment4/data/glove/glove.840B.300d.txt.cache\n",
      "New vocab size: 20693\n",
      "Found 8758 unknown tokens.\n",
      "Found 1255 unknown tokens.\n",
      "Found 2103 unknown tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adriencogny/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Users/adriencogny/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=0 Tr-Loss=1.6004887819290161 Tr-Acc=0.25\n",
      "Ev-Err=0.7510495720975294\n",
      "Checkpointing model step=0 best_val_err=0.7510495720975294.\n",
      "Step=100 Tr-Loss=1.1851764917373657 Tr-Acc=0.59375\n",
      "Step=200 Tr-Loss=1.437263011932373 Tr-Acc=0.46875\n",
      "Step=300 Tr-Loss=1.6297235488891602 Tr-Acc=0.34375\n",
      "Step=400 Tr-Loss=1.2242522239685059 Tr-Acc=0.53125\n",
      "Step=500 Tr-Loss=1.230650782585144 Tr-Acc=0.46875\n",
      "Step=600 Tr-Loss=1.3034064769744873 Tr-Acc=0.4375\n",
      "Step=700 Tr-Loss=1.0041857957839966 Tr-Acc=0.625\n",
      "Step=800 Tr-Loss=1.3313339948654175 Tr-Acc=0.46875\n",
      "Step=900 Tr-Loss=1.1603057384490967 Tr-Acc=0.5625\n",
      "Step=1000 Tr-Loss=0.9664885401725769 Tr-Acc=0.65625\n",
      "Ev-Err=0.4517600516712417\n",
      "Checkpointing model step=1000 best_val_err=0.4517600516712417.\n",
      "Step=1100 Tr-Loss=1.1403368711471558 Tr-Acc=0.46875\n",
      "Step=1200 Tr-Loss=1.2545894384384155 Tr-Acc=0.53125\n",
      "Step=1300 Tr-Loss=1.1906952857971191 Tr-Acc=0.5\n",
      "Step=1400 Tr-Loss=0.804442822933197 Tr-Acc=0.71875\n",
      "Step=1500 Tr-Loss=1.212820053100586 Tr-Acc=0.5625\n",
      "Step=1600 Tr-Loss=1.4982759952545166 Tr-Acc=0.46875\n",
      "Step=1700 Tr-Loss=1.4099071025848389 Tr-Acc=0.375\n",
      "Step=1800 Tr-Loss=1.3057204484939575 Tr-Acc=0.46875\n",
      "Step=1900 Tr-Loss=0.9252058267593384 Tr-Acc=0.6875\n",
      "Step=2000 Tr-Loss=1.214643955230713 Tr-Acc=0.4375\n",
      "Ev-Err=0.4322218633941547\n",
      "Checkpointing model step=2000 best_val_err=0.4322218633941547.\n",
      "Step=2100 Tr-Loss=1.198217749595642 Tr-Acc=0.5625\n",
      "Step=2200 Tr-Loss=1.019516110420227 Tr-Acc=0.625\n",
      "Step=2300 Tr-Loss=1.11185622215271 Tr-Acc=0.59375\n",
      "Step=2400 Tr-Loss=1.0344105958938599 Tr-Acc=0.71875\n",
      "Step=2500 Tr-Loss=1.4856423139572144 Tr-Acc=0.28125\n",
      "Step=2600 Tr-Loss=1.5362279415130615 Tr-Acc=0.4375\n",
      "Step=2700 Tr-Loss=1.1582602262496948 Tr-Acc=0.46875\n",
      "Step=2800 Tr-Loss=1.2337056398391724 Tr-Acc=0.5625\n",
      "Step=2900 Tr-Loss=1.1346296072006226 Tr-Acc=0.5\n",
      "Step=3000 Tr-Loss=1.2470004558563232 Tr-Acc=0.53125\n",
      "Ev-Err=0.42087841110931695\n",
      "Checkpointing model step=3000 best_val_err=0.42087841110931695.\n",
      "Step=3100 Tr-Loss=1.2450058460235596 Tr-Acc=0.625\n",
      "Step=3200 Tr-Loss=1.2549753189086914 Tr-Acc=0.5\n",
      "Step=3300 Tr-Loss=1.182265043258667 Tr-Acc=0.5625\n",
      "Step=3400 Tr-Loss=1.314652919769287 Tr-Acc=0.5625\n",
      "Step=3500 Tr-Loss=1.1173425912857056 Tr-Acc=0.5\n",
      "Step=3600 Tr-Loss=1.2451571226119995 Tr-Acc=0.59375\n",
      "Step=3700 Tr-Loss=1.0343940258026123 Tr-Acc=0.625\n",
      "Step=3800 Tr-Loss=1.6305265426635742 Tr-Acc=0.28125\n",
      "Step=3900 Tr-Loss=1.3936926126480103 Tr-Acc=0.4375\n",
      "Step=4000 Tr-Loss=0.9546019434928894 Tr-Acc=0.65625\n",
      "Ev-Err=0.41498466010011303\n",
      "Checkpointing model step=4000 best_val_err=0.41498466010011303.\n",
      "Step=4100 Tr-Loss=1.2541810274124146 Tr-Acc=0.46875\n",
      "Step=4200 Tr-Loss=1.1192448139190674 Tr-Acc=0.5625\n",
      "Step=4300 Tr-Loss=1.1402065753936768 Tr-Acc=0.5625\n",
      "Step=4400 Tr-Loss=1.3925819396972656 Tr-Acc=0.375\n",
      "Step=4500 Tr-Loss=1.0302479267120361 Tr-Acc=0.625\n",
      "Step=4600 Tr-Loss=1.3403747081756592 Tr-Acc=0.4375\n",
      "Step=4700 Tr-Loss=1.0724421739578247 Tr-Acc=0.65625\n",
      "Step=4800 Tr-Loss=1.3044742345809937 Tr-Acc=0.4375\n",
      "Step=4900 Tr-Loss=1.1794332265853882 Tr-Acc=0.5\n",
      "Step=5000 Tr-Loss=0.9601597785949707 Tr-Acc=0.5625\n",
      "Ev-Err=0.4103826901340223\n",
      "Checkpointing model step=5000 best_val_err=0.4103826901340223.\n",
      "Step=5100 Tr-Loss=1.2441439628601074 Tr-Acc=0.4375\n",
      "Step=5200 Tr-Loss=1.2120623588562012 Tr-Acc=0.5\n",
      "Step=5300 Tr-Loss=1.0344669818878174 Tr-Acc=0.5\n",
      "Step=5400 Tr-Loss=1.1345865726470947 Tr-Acc=0.59375\n",
      "Step=5500 Tr-Loss=1.0944101810455322 Tr-Acc=0.59375\n",
      "Step=5600 Tr-Loss=1.3608171939849854 Tr-Acc=0.53125\n",
      "Step=5700 Tr-Loss=1.2498399019241333 Tr-Acc=0.5\n",
      "Step=5800 Tr-Loss=1.126286506652832 Tr-Acc=0.53125\n",
      "Step=5900 Tr-Loss=1.245091438293457 Tr-Acc=0.40625\n",
      "Step=6000 Tr-Loss=1.6135263442993164 Tr-Acc=0.4375\n",
      "Ev-Err=0.41131115775875987\n",
      "Step=6100 Tr-Loss=1.0433131456375122 Tr-Acc=0.71875\n",
      "Step=6200 Tr-Loss=1.089004635810852 Tr-Acc=0.5625\n",
      "Step=6300 Tr-Loss=0.8982154726982117 Tr-Acc=0.6875\n",
      "Step=6400 Tr-Loss=1.2769668102264404 Tr-Acc=0.34375\n",
      "Step=6500 Tr-Loss=1.1015244722366333 Tr-Acc=0.46875\n",
      "Step=6600 Tr-Loss=1.2776753902435303 Tr-Acc=0.46875\n",
      "Step=6700 Tr-Loss=1.2297625541687012 Tr-Acc=0.5625\n",
      "Step=6800 Tr-Loss=1.0866961479187012 Tr-Acc=0.625\n",
      "Step=6900 Tr-Loss=1.1227456331253052 Tr-Acc=0.65625\n",
      "Step=7000 Tr-Loss=0.8585274815559387 Tr-Acc=0.65625\n",
      "Ev-Err=0.40763765541740676\n",
      "Checkpointing model step=7000 best_val_err=0.40763765541740676.\n",
      "Step=7100 Tr-Loss=1.2045704126358032 Tr-Acc=0.5\n",
      "Step=7200 Tr-Loss=0.9933621287345886 Tr-Acc=0.5\n",
      "Step=7300 Tr-Loss=1.1205765008926392 Tr-Acc=0.5\n",
      "Step=7400 Tr-Loss=1.0542868375778198 Tr-Acc=0.59375\n",
      "Step=7500 Tr-Loss=1.1110576391220093 Tr-Acc=0.53125\n",
      "Step=7600 Tr-Loss=1.0837428569793701 Tr-Acc=0.53125\n",
      "Step=7700 Tr-Loss=1.0910115242004395 Tr-Acc=0.5625\n",
      "Step=7800 Tr-Loss=0.9205889105796814 Tr-Acc=0.625\n",
      "Step=7900 Tr-Loss=0.9809975028038025 Tr-Acc=0.59375\n",
      "Step=8000 Tr-Loss=1.5078480243682861 Tr-Acc=0.5\n",
      "Ev-Err=0.4027127401905377\n",
      "Checkpointing model step=8000 best_val_err=0.4027127401905377.\n",
      "Step=8100 Tr-Loss=0.9042191505432129 Tr-Acc=0.65625\n",
      "Step=8200 Tr-Loss=0.8910004496574402 Tr-Acc=0.59375\n",
      "Step=8300 Tr-Loss=1.2860100269317627 Tr-Acc=0.4375\n",
      "Step=8400 Tr-Loss=1.2546857595443726 Tr-Acc=0.46875\n",
      "Step=8500 Tr-Loss=1.2716835737228394 Tr-Acc=0.53125\n",
      "Step=8600 Tr-Loss=1.0159395933151245 Tr-Acc=0.65625\n",
      "Step=8700 Tr-Loss=1.2354284524917603 Tr-Acc=0.53125\n",
      "Step=8800 Tr-Loss=1.1529656648635864 Tr-Acc=0.53125\n",
      "Step=8900 Tr-Loss=1.0496289730072021 Tr-Acc=0.6875\n",
      "Step=9000 Tr-Loss=1.029465913772583 Tr-Acc=0.59375\n",
      "Ev-Err=0.40214758598417566\n",
      "Checkpointing model step=9000 best_val_err=0.40214758598417566.\n",
      "Step=9100 Tr-Loss=1.1249244213104248 Tr-Acc=0.5625\n",
      "Step=9200 Tr-Loss=1.0223451852798462 Tr-Acc=0.5625\n",
      "Step=9300 Tr-Loss=0.8917374610900879 Tr-Acc=0.65625\n",
      "Step=9400 Tr-Loss=1.1420009136199951 Tr-Acc=0.53125\n",
      "Step=9500 Tr-Loss=0.8809335827827454 Tr-Acc=0.8125\n",
      "Step=9600 Tr-Loss=1.036543846130371 Tr-Acc=0.5625\n",
      "Step=9700 Tr-Loss=1.3600523471832275 Tr-Acc=0.46875\n",
      "Step=9800 Tr-Loss=1.0484415292739868 Tr-Acc=0.625\n",
      "Step=9900 Tr-Loss=1.1073827743530273 Tr-Acc=0.53125\n",
      "Step=10000 Tr-Loss=0.9927789568901062 Tr-Acc=0.5625\n",
      "Ev-Err=0.39911997416437917\n",
      "Checkpointing model step=10000 best_val_err=0.39911997416437917.\n",
      "Step=10100 Tr-Loss=1.0372121334075928 Tr-Acc=0.65625\n",
      "Step=10200 Tr-Loss=1.1935232877731323 Tr-Acc=0.59375\n",
      "Step=10300 Tr-Loss=1.0109846591949463 Tr-Acc=0.53125\n",
      "Step=10400 Tr-Loss=0.9847455620765686 Tr-Acc=0.59375\n",
      "Step=10500 Tr-Loss=1.1101717948913574 Tr-Acc=0.59375\n",
      "Step=10600 Tr-Loss=1.0175937414169312 Tr-Acc=0.65625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-1f9c6dcd8fd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-1f9c6dcd8fd9>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(options)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main\n",
    "\n",
    "def run_validation(model, dataset, options):\n",
    "  err = 0\n",
    "  count = 0\n",
    "  for data, labels, _ in batch_iterator(dataset, options.batch_size, forever=False):\n",
    "    outp = model(Variable(data))\n",
    "    loss = nn.NLLLoss()(F.log_softmax(outp), Variable(labels))\n",
    "    acc = (outp.data.max(1)[1] == labels).sum() / data.shape[0]\n",
    "    err += (1-acc) * data.shape[0]\n",
    "    count += data.shape[0]\n",
    "  err = err / count\n",
    "  print('Ev-Err={}'.format(err))\n",
    "  return err\n",
    "\n",
    "\n",
    "def run_test(model, dataset, options):\n",
    "  print('Writing predictions to {}'.format(os.path.abspath(options.predictions)))\n",
    "\n",
    "  preds_dict = dict()\n",
    "\n",
    "  for data, _, example_ids in batch_iterator(dataset, options.batch_size, forever=False):\n",
    "    outp = model(Variable(data))\n",
    "    preds = outp.data.max(1)[1]\n",
    "\n",
    "    for id, pred in zip(example_ids, preds):\n",
    "      preds_dict[id] = pred\n",
    "\n",
    "  with open(options.predictions, 'w') as f:\n",
    "    for id, pred in preds_dict.items():\n",
    "      f.write('{}|{}\\n'.format(id, pred))\n",
    "\n",
    "\n",
    "def run(options):\n",
    "  train_data, validation_data, test_data, vocab, embeddings = \\\n",
    "    load_data_and_embeddings(options.data, options.ids, options.embeddings)\n",
    "  model = BagOfWordsModel(embeddings) # change this if we create a different class names than bag of words\n",
    "  model = CNNClassifier(len(vocab), len(embeddings), output_size)\n",
    "  opt = optim.SGD(model.parameters(), lr=3e-4)\n",
    "  \n",
    "  step = 0\n",
    "  best_val_err = 1\n",
    "\n",
    "  if options.eval_only_mode:\n",
    "    step, best_val_err = load_model(model, opt, options.model)\n",
    "    print('Model loaded from {}\\nstep={} best_val_err={}'.format(options.model, step, best_val_err))\n",
    "    run_test(model, test_data, options)\n",
    "    sys.exit()\n",
    "  \n",
    "  for data, labels, _ in batch_iterator(train_data, options.batch_size, forever=True):\n",
    "#     print(data)\n",
    "    outp = model(Variable(data))\n",
    "    loss = nn.NLLLoss()(F.log_softmax(outp), Variable(labels))\n",
    "    acc = (outp.data.max(1)[1] == labels).sum() / data.shape[0]\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    if step % options.log_every == 0:\n",
    "      print('Step={} Tr-Loss={} Tr-Acc={}'.format(step, loss.data[0], acc))\n",
    "      \n",
    "    if step % options.eval_every == 0:\n",
    "      val_err = run_validation(model, validation_data, options)\n",
    "      \n",
    "      # early stopping\n",
    "      if val_err < best_val_err:\n",
    "        best_val_err = val_err\n",
    "        print('Checkpointing model step={} best_val_err={}.'.format(step, best_val_err))\n",
    "        checkpoint_model(step, val_err, model, opt, options.model)\n",
    "    \n",
    "    step += 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    \n",
    "#   parser = argparse.ArgumentParser()\n",
    "#   parser.add_argument('--ids', default=mydir, type=str)\n",
    "#   parser.add_argument('--data', default=os.path.expanduser('~/data/stanfordSentimentTreebank'), type=str)\n",
    "#   parser.add_argument('--embeddings', default=, type=str)\n",
    "#   parser.add_argument('--model', default=os.path.join(mydir, 'model.ckpt'), type=str)\n",
    "#   parser.add_argument('--predictions', default=os.path.join(mydir, 'predictions.txt'), type=str)\n",
    "#   parser.add_argument('--log_every', default=100, type=int)\n",
    "#   parser.add_argument('--eval_every', default=1000, type=int)\n",
    "#   parser.add_argument('--batch_size', default=32, type=int)\n",
    "#   parser.add_argument('--eval_only_mode', action='store_true')\n",
    "#   options = parser.parse_args()\n",
    "\n",
    "#   print(json.dumps(options.__dict__, sort_keys=True, indent=4))\n",
    "    \n",
    "    options = Namespace(\n",
    "        ids=mydir,\n",
    "        data=os.path.expanduser(mydir + '/data/stanfordSentimentTreebank'),\n",
    "        embeddings = os.path.expanduser(mydir +'/data/glove/glove.840B.300d.txt'),\n",
    "        model = os.path.join(mydir, 'model.ckpt'),\n",
    "        predictions = os.path.join(mydir, 'predictions.txt'),\n",
    "        log_every = 100,\n",
    "        batch_size = 32,\n",
    "        eval_only_mode = False,\n",
    "        eval_every=1000\n",
    "    )\n",
    "    \n",
    "    \n",
    "#     {\n",
    "#         ids:mydir,\n",
    "#         data: os.path.expanduser(mydir + '/data/stanfordSentimentTreebank'),\n",
    "#         'embeddings':0\n",
    "#     }\n",
    "    print(options)\n",
    "    \n",
    "    run(options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
