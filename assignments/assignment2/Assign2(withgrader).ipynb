{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import itertools\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Additional helper functions required for the code to run:\n",
    "# This function defines the column headers for the data frames\n",
    "def getColumnHeaders():\n",
    "    return pd.Series(data=['label','integer_1','integer_2','integer_3',\n",
    "                                 'integer_4','integer_5','integer_6','integer_7','integer_8','integer_9',\n",
    "                                 'integer_10','integer_11','integer_12','integer_13','categorical_1',\n",
    "                                 'categorical_2','categorical_3','categorical_4','categorical_5','categorical_6',\n",
    "                                 'categorical_7','categorical_8','categorical_9','categorical_10','categorical_11',\n",
    "                                 'categorical_12','categorical_13','categorical_14','categorical_15','categorical_16',\n",
    "                                 'categorical_17','categorical_18','categorical_19','categorical_20','categorical_21',\n",
    "                                 'categorical_22','categorical_23','categorical_24','categorical_25','categorical_26','Index'])\n",
    "\n",
    "# this function generateNIndecesFrom takes a range of Indeces and randomly takes n of them, returns a pandas Series object\n",
    "def generateNIndecesFrom(n, rangeOfIndeces):\n",
    "    # print(\"Generating \" + str(n) + \" indeces from range\")\n",
    "    allIndeces = random.sample(rangeOfIndeces, n)\n",
    "    allIndeces = pd.Series(data = allIndeces)\n",
    "    allIndeces = allIndeces.sort_values().reset_index().drop(['index'],axis=1)\n",
    "    allIndeces.columns = ['Index'];\n",
    "    return allIndeces\n",
    "\n",
    "# This function takes in a dataframe and computes statistics and histograms for all columns that are not 'label', 'Index' or 'Unnamed: 0'\n",
    "# It was used in part 2.2 to generate the histograms and statistics.\n",
    "# It can for example be placed at the end of the read_data method and be passed one of the datasets to compute its statistics and histograms\n",
    "def generateSummaryStatsAndHists(train1M):\n",
    "    SummaryStats = pd.DataFrame()\n",
    "    for col in train1M.columns:\n",
    "        if (col != 'label' and col != 'Index' and col != 'Unnamed: 0'):\n",
    "\n",
    "            train1M[col].value_counts().plot(kind='hist',title=col, bins=100)\n",
    "            plt.savefig(col)\n",
    "            plt.show()\n",
    "            plt.gcf().clear()\n",
    "            if (train1M[col].dtype != 'O'):\n",
    "                SummaryStats[col] = train1M[col].describe()   \n",
    "    # SummaryStats.head()\n",
    "    SummaryStats.to_csv('integerStats.csv')\n",
    "    return SummaryStats\n",
    "\n",
    "\n",
    "# the generateSubSet function takes in a file, a dataFrame to put the data in as well as index values which should be extracted,\n",
    "# a number of rows per itteration (this is used to not overload the memory) , total number of rows in the file, the column headers for the new dataframe\n",
    "def generateSubSet(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers):\n",
    "    totalNumIterations = int(totalNumRows/numRowsPerItteration)\n",
    "    # print(\"Number of itterations = \" + str(totalNumIterations))\n",
    "    totalNumRowsTraversed = 0\n",
    "    prevsize = 0\n",
    "    for i in range(totalNumIterations + 1):\n",
    "#         \n",
    "#         print(\"Itteration number: \" + str(i))\n",
    "#         print(\"skipRows: \" + str(i * numRowsPerItteration))\n",
    "#         print(\"Read in : \" + str(numRowsPerItteration))\n",
    "        curData = pd.read_table(file,skiprows = i * numRowsPerItteration, nrows = numRowsPerItteration,header=None)\n",
    "        curData.index = [i for i in range(i*numRowsPerItteration,i*numRowsPerItteration + curData.shape[0])]\n",
    "        totalNumRowsTraversed = totalNumRowsTraversed + curData.shape[0]\n",
    "        clear_output()\n",
    "#         print(curData.head())\n",
    "#         print(curData.shape)\n",
    "#         print(curData.index.shape)\n",
    "\n",
    "        curData['Index'] = curData.index\n",
    "        curData.columns = column_headers\n",
    "        \n",
    "        curIndexRange = indexValues['Index'][(indexValues['Index'] < (i*numRowsPerItteration + numRowsPerItteration)) & (indexValues['Index'] > (i*numRowsPerItteration-1))]\n",
    "#         print(curIndexRange)\n",
    "#         print(curData.head())\n",
    "#         print(curData['Index'])\n",
    "#         print(curData['Index'].isin(curIndexRange))\n",
    "        curData = curData[curData['Index'].isin(list(curIndexRange))] # this line isn't working for some reason\n",
    "#         print(curData.head())\n",
    "        \n",
    "        dataFrame = pd.concat([dataFrame,curData])\n",
    "        \n",
    "                \n",
    "        \n",
    "#         print(dataFrame.head())\n",
    "#         print(curData.head())\n",
    "        \n",
    "        \n",
    "        print(\"Extraction Stats: \" + str(dataFrame.shape[0]) + \" percent: \" + str(dataFrame.shape[0] / indexValues.shape[0] * 100) + \"%\")\n",
    "        print(\"Document Stats: \" + str(totalNumRowsTraversed) + \" percent: \" + str(totalNumRowsTraversed/totalNumRows*100) + \"%\")\n",
    "        if (dataFrame.shape[0] - prevsize) > 500000:\n",
    "            prevsize = dataFrame.shape[0]\n",
    "#             dataFrame.to_csv(frameSaveName)\n",
    "#         elif dataFrame.shape[0] == indexValues.shape[0]:\n",
    "#             print(\"Finished with the data collection\")\n",
    "# #             dataFrame.to_csv(frameSaveName)\n",
    "#             break\n",
    "    # print(\"Extraction is Done, now saving frame\")        \n",
    "    return dataFrame\n",
    "\n",
    "# This method generates is a wrapper around the generateSubset to generate the subset and save the dataframe to a csv file (for being able to make use of it after)\n",
    "def generateAndSaveSubset(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers,frameSaveName):\n",
    "    dataFrame = generateSubSet(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers)\n",
    "    dataFrame.to_csv(frameSaveName)\n",
    "    return dataFrame\n",
    "\n",
    "def read_data(data_path, train_path, validation_path, test_path):\n",
    "\n",
    "    print(data_path)\n",
    "    print(train_path)\n",
    "    print(validation_path)\n",
    "    print(test_path)\n",
    "    \n",
    "    #get the ids\n",
    "    try:\n",
    "        trainIndeces = pd.read_csv(train_path, header = None)\n",
    "        validationIndeces = pd.read_csv(validation_path, header = None)\n",
    "        testingIndeces = pd.read_csv(test_path, header = None)\n",
    "    except:\n",
    "        print(\"There were not 1000000 data points\")\n",
    "        trainIndeces = generateNIndecesFrom(1000000,list(twoMIndeces['Index']))\n",
    "        trainIndeces.to_csv('train_ids.txt',index=False,header=False)\n",
    "\n",
    "        remainingIndeces = twoMIndeces['Index'][~twoMIndeces['Index'].isin(trainIndeces.values)]\n",
    "        validationIndeces = generateNIndecesFrom(250000,list(remainingIndeces))\n",
    "        validationIndeces.to_csv('validation_ids.txt',index=False,header=False)\n",
    "\n",
    "        testingIndeces = twoMIndeces['Index'][~(twoMIndeces['Index'].isin(trainIndeces.values) | twoMIndeces['Index'].isin(validationIndeces.values))]\n",
    "        testingIndeces = generateNIndecesFrom(750000,list(testingIndeces))\n",
    "        testingIndeces.to_csv('test_ids.txt',index=False,header=False)\n",
    "    \n",
    "    trainIndeces.columns = ['Index']\n",
    "    validationIndeces.columns = ['Index']\n",
    "    testingIndeces.columns = ['Index']\n",
    "#     print(trainIndeces.head())\n",
    "#     print(validationIndeces.head())\n",
    "#     print(testingIndeces.head())\n",
    "    \n",
    "    # Generate the actual data files\n",
    "    column_headers = getColumnHeaders()\n",
    "    # print(\"No 1M collection\")\n",
    "    train1M = pd.DataFrame()\n",
    "    train1M = generateSubSet(data_path,train1M,trainIndeces,4000000,46000000,column_headers)\n",
    "#     print(train1M.head())\n",
    "\n",
    "    # print(\"No 250k collection\")\n",
    "    validation250k = pd.DataFrame()\n",
    "    validation250k = generateSubSet(data_path,validation250k,validationIndeces,4000000,46000000,column_headers)\n",
    "\n",
    "\n",
    "    # print(\"No 750k collection\")\n",
    "    test750k = pd.DataFrame()\n",
    "    test750k = generateSubSet(data_path,test750k,testingIndeces,4000000,46000000,column_headers)\n",
    "    \n",
    "    \n",
    "    print(train1M.shape)\n",
    "    print(validation250k.shape)\n",
    "    print(test750k.shape)\n",
    "\n",
    "    # train_data, train_target = np.zeros((1000000, 39)), np.zeros((1000000,))\n",
    "    # validation_data, validation_target = np.zeros((250000, 39)), np.zeros((250000,))\n",
    "    # test_data, test_target = np.zeros((750000, 39)), np.zeros((750000,))\n",
    "\n",
    "\n",
    "    return train1M[train1M.columns[1:40]].values, train1M['label'].values, validation250k[validation250k.columns[1:40]].values, validation250k['label'].values, test750k[test750k.columns[1:40]].values, test750k['label'].values\n",
    "#     return \n",
    "\n",
    "def preprocess_int_data(data, features):\n",
    "    n = len([f for f in features if f < 13])\n",
    "    print(n)\n",
    "    print(data)\n",
    "    dataFrame = pd.DataFrame(data)\n",
    "    print(dataFrame.head())\n",
    "    return np.zeros((data.shape[0], n))\n",
    "\n",
    "\n",
    "def preprocess_cat_data(data, features, preprocess):\n",
    "    print(data)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction Stats: 250000 percent: 100.0%\n",
      "Document Stats: 45840617 percent: 99.6535152173913%\n"
     ]
    }
   ],
   "source": [
    "def check_output_features(fs):\n",
    "    assert len(fs) < 39\n",
    "\n",
    "\n",
    "def check_output_read_data(data, target, n):\n",
    "    assert data.shape[0] == n\n",
    "    assert target.shape[0] == n\n",
    "\n",
    "\n",
    "def check_output_preprocess(preprocess):\n",
    "    assert (preprocess == 'onehot') or (preprocess == 'rate') or (preprocess == 'tfidf')\n",
    "\n",
    "\n",
    "def check_output_preprocess_int_data(data, fs):\n",
    "    n = len([f for f in fs if f < 13])\n",
    "    assert data.shape[1] == n\n",
    "\n",
    "\n",
    "def check_output_preprocess_cat_data(data, fs, preprocess):\n",
    "    pass\n",
    "\n",
    "\n",
    "def read_features(path):\n",
    "    features = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            features.append(int(line.strip()))\n",
    "    return features\n",
    "\n",
    "\n",
    "def read_preprocess(path):\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            return line.strip()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = './dac/train.txt'\n",
    "    train = 'train_ids.txt'\n",
    "    validation = 'validation_ids.txt'\n",
    "    test = 'test_ids.txt'\n",
    "    features = 'features.txt'\n",
    "    preprocess = 'preprocess.txt'\n",
    "\n",
    "    train_data, train_target, validation_data, validation_target, test_data, test_target = \\\n",
    "        read_data(data, train, validation, test)\n",
    "\n",
    "    check_output_read_data(train_data, train_target, 1000000)\n",
    "    check_output_read_data(validation_data, validation_target, 250000)\n",
    "    check_output_read_data(test_data, test_target, 750000)\n",
    "\n",
    "    features = read_features(options.features)\n",
    "\n",
    "    check_output_features(features)\n",
    "\n",
    "    preprocess = read_preprocess(options.preprocess)\n",
    "\n",
    "    check_output_preprocess(prepr3ocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_int_data = preprocess_int_data(train_data, features)\n",
    "validation_int_data = preprocess_int_data(validation_data, features)\n",
    "test_int_data = preprocess_int_data(test_data, features)\n",
    "\n",
    "check_output_preprocess_int_data(train_int_data, features)\n",
    "check_output_preprocess_int_data(validation_int_data, features)\n",
    "check_output_preprocess_int_data(test_int_data, features)\n",
    "\n",
    "train_cat_data = preprocess_cat_data(train_data, features, preprocess)\n",
    "validation_cat_data = preprocess_cat_data(validation_data, features, preprocess)\n",
    "test_cat_data = preprocess_cat_data(test_data, features, preprocess)\n",
    "\n",
    "check_output_preprocess_cat_data(train_cat_data, features, preprocess)\n",
    "check_output_preprocess_cat_data(validation_cat_data, features, preprocess)\n",
    "check_output_preprocess_cat_data(test_cat_data, features, preprocess)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
