{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals import joblib \n",
    "\n",
    "\n",
    "from sys import getsizeof\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Additional helper functions required for the code to run:\n",
    "# This function defines the column headers for the data frames\n",
    "def getColumnHeaders():\n",
    "    return pd.Series(data=['label','integer_1','integer_2','integer_3',\n",
    "                                 'integer_4','integer_5','integer_6','integer_7','integer_8','integer_9',\n",
    "                                 'integer_10','integer_11','integer_12','integer_13','categorical_1',\n",
    "                                 'categorical_2','categorical_3','categorical_4','categorical_5','categorical_6',\n",
    "                                 'categorical_7','categorical_8','categorical_9','categorical_10','categorical_11',\n",
    "                                 'categorical_12','categorical_13','categorical_14','categorical_15','categorical_16',\n",
    "                                 'categorical_17','categorical_18','categorical_19','categorical_20','categorical_21',\n",
    "                                 'categorical_22','categorical_23','categorical_24','categorical_25','categorical_26','Index'])\n",
    "\n",
    "def getDataHeaders():\n",
    "    return pd.Series(data=['integer_1','integer_2','integer_3',\n",
    "                                 'integer_4','integer_5','integer_6','integer_7','integer_8','integer_9',\n",
    "                                 'integer_10','integer_11','integer_12','integer_13','categorical_1',\n",
    "                                 'categorical_2','categorical_3','categorical_4','categorical_5','categorical_6',\n",
    "                                 'categorical_7','categorical_8','categorical_9','categorical_10','categorical_11',\n",
    "                                 'categorical_12','categorical_13','categorical_14','categorical_15','categorical_16',\n",
    "                                 'categorical_17','categorical_18','categorical_19','categorical_20','categorical_21',\n",
    "                                 'categorical_22','categorical_23','categorical_24','categorical_25','categorical_26'])\n",
    "\n",
    "# this function generateNIndecesFrom takes a range of Indeces and randomly takes n of them, returns a pandas Series object\n",
    "def generateNIndecesFrom(n, rangeOfIndeces):\n",
    "    # print(\"Generating \" + str(n) + \" indeces from range\")\n",
    "    allIndeces = random.sample(rangeOfIndeces, n)\n",
    "    allIndeces = pd.Series(data = allIndeces)\n",
    "    allIndeces = allIndeces.sort_values().reset_index().drop(['index'],axis=1)\n",
    "    allIndeces.columns = ['Index'];\n",
    "    return allIndeces\n",
    "\n",
    "# This function takes in a dataframe and computes statistics and histograms for all columns that are not 'label', 'Index' or 'Unnamed: 0'\n",
    "# It was used in part 2.2 to generate the histograms and statistics.\n",
    "# It can for example be placed at the end of the read_data method and be passed one of the datasets to compute its statistics and histograms\n",
    "def generateSummaryStatsAndHists(train1M):\n",
    "    SummaryStats = pd.DataFrame()\n",
    "    for col in train1M.columns:\n",
    "        if (col != 'label' and col != 'Index' and col != 'Unnamed: 0'):\n",
    "\n",
    "            train1M[col][train1M['label'] == 0].value_counts().plot(kind='hist',title=col, bins=100,label='0s')\n",
    "            train1M[col][train1M['label'] == 1].value_counts().plot(kind='hist',title=col, bins=100,label='1s')\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.savefig(col)\n",
    "#             plt.show()\n",
    "            plt.gcf().clear()\n",
    "            if (train1M[col].dtype != 'O'):\n",
    "                SummaryStats[col] = train1M[col].describe()   \n",
    "    # SummaryStats.head()\n",
    "    SummaryStats.to_csv('integerStats.csv')\n",
    "    return SummaryStats\n",
    "\n",
    "\n",
    "# the generateSubSet function takes in a file, a dataFrame to put the data in as well as index values which should be extracted,\n",
    "# a number of rows per itteration (this is used to not overload the memory) , total number of rows in the file, the column headers for the new dataframe\n",
    "def generateSubSet(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers):\n",
    "    totalNumIterations = int(totalNumRows/numRowsPerItteration)\n",
    "    # print(\"Number of itterations = \" + str(totalNumIterations))\n",
    "    totalNumRowsTraversed = 0\n",
    "    prevsize = 0\n",
    "    for i in range(totalNumIterations + 1):\n",
    "\n",
    "        curData = pd.read_table(file,skiprows = i * numRowsPerItteration, nrows = numRowsPerItteration,header=None)\n",
    "        curData.index = [i for i in range(i*numRowsPerItteration,i*numRowsPerItteration + curData.shape[0])]\n",
    "        totalNumRowsTraversed = totalNumRowsTraversed + curData.shape[0]\n",
    "        \n",
    "\n",
    "        curData['Index'] = curData.index\n",
    "        curData.columns = column_headers\n",
    "        \n",
    "        curIndexRange = indexValues['Index'][(indexValues['Index'] < (i*numRowsPerItteration + numRowsPerItteration)) & (indexValues['Index'] > (i*numRowsPerItteration-1))]\n",
    "        curData = curData[curData['Index'].isin(list(curIndexRange))]\n",
    "        \n",
    "        dataFrame = pd.concat([dataFrame,curData])\n",
    "        \n",
    "#         clear_output()\n",
    "        print(\"Extraction Stats: \" + str(dataFrame.shape[0]) + \" percent: \" + str(dataFrame.shape[0] / indexValues.shape[0] * 100) + \"%\")\n",
    "#         print(\"Document Stats: \" + str(totalNumRowsTraversed) + \" percent: \" + str(totalNumRowsTraversed/totalNumRows*100) + \"%\")\n",
    "        if (dataFrame.shape[0] - prevsize) > 500000:\n",
    "            prevsize = dataFrame.shape[0]\n",
    "      \n",
    "    return dataFrame\n",
    "\n",
    "# This method generates is a wrapper around the generateSubset to generate the subset and save the dataframe to a csv file (for being able to make use of it after)\n",
    "def generateAndSaveSubset(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers,frameSaveName):\n",
    "    dataFrame = generateSubSet(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers)\n",
    "    dataFrame.to_csv(frameSaveName)\n",
    "    return dataFrame\n",
    "\n",
    "# This method generates the categorical data required to then apply one hot encoding on the entire dataset\n",
    "def generateCategoricalData(train1M):\n",
    "    #change to categorical\n",
    "    for col in train1M.columns[14:40]:\n",
    "        train1M[col] = train1M[col].astype('category')\n",
    "        \n",
    "        #get only the top 10 categories with highest count\n",
    "        averageNumber = train1M[col].value_counts().mean()\n",
    "#         print(averageNumber)\n",
    "        counts = train1M[col].value_counts()\n",
    "# #         print(counts)\n",
    "#         if (averageNumber < 10):\n",
    "#             # if the average counte is less than 10, just take the first 20 categories\n",
    "#             topFeatures = train1M[col].value_counts()[:20].index\n",
    "#         else:\n",
    "        topFeatures = train1M[col].value_counts()[train1M[col].value_counts() > averageNumber].index\n",
    "\n",
    "#         print(topFeatures)\n",
    "        \n",
    "        # add the dummy category\n",
    "#         train1M[col].cat.add_categories(new_categories = 'Dummy',inplace = True)\n",
    "        categories = pd.Series(topFeatures)\n",
    "        print(categories.shape)\n",
    "        categories.to_csv(str(col)+'_features.csv',header = False)\n",
    "        #save the categories for each column\n",
    "        #then we can set the categegories for each column\n",
    "        # and when we get dummies from pandas we have a one hot encoding that is consistent accross\n",
    "        # -> get_dummies() method does one hot encoding\n",
    "\n",
    "# This methods takes the training set and creates a scaler that is fit to the integer columns of the training set then saves\n",
    "# The model to file for future retrieval.\n",
    "def preProcessIntsAndSave(dataFrame,fileName):\n",
    "    dataFrame[dataFrame.columns[1:14]] = dataFrame[dataFrame.columns[1:14]].fillna(0)\n",
    "    dataFrame[dataFrame.columns[1:14]] = dataFrame[dataFrame.columns[1:14]].replace(-1,0)\n",
    "    dataFrame[dataFrame.columns[1:14]] = dataFrame[dataFrame.columns[1:14]].replace(-2,0)\n",
    "    \n",
    "    curScaler = StandardScaler()\n",
    "    curScaler.fit(dataFrame[dataFrame.columns[1:14]])\n",
    "    joblib.dump(curScaler, fileName)\n",
    "    return\n",
    "        \n",
    "def read_data(data_path, train_path, validation_path, test_path):\n",
    "\n",
    "    print(data_path)\n",
    "    print(train_path)\n",
    "    print(validation_path)\n",
    "    print(test_path)\n",
    "    \n",
    "    #get the ids\n",
    "    try:\n",
    "        trainIndeces = pd.read_csv(train_path, header = None)\n",
    "        validationIndeces = pd.read_csv(validation_path, header = None)\n",
    "        testingIndeces = pd.read_csv(test_path, header = None)\n",
    "    except:\n",
    "        print(\"There were not 1000000 data points\")\n",
    "        trainIndeces = generateNIndecesFrom(1000000,list(twoMIndeces['Index']))\n",
    "        trainIndeces.to_csv('train_ids.txt',index=False,header=False)\n",
    "\n",
    "        remainingIndeces = twoMIndeces['Index'][~twoMIndeces['Index'].isin(trainIndeces.values)]\n",
    "        validationIndeces = generateNIndecesFrom(250000,list(remainingIndeces))\n",
    "        validationIndeces.to_csv('validation_ids.txt',index=False,header=False)\n",
    "\n",
    "        testingIndeces = twoMIndeces['Index'][~(twoMIndeces['Index'].isin(trainIndeces.values) | twoMIndeces['Index'].isin(validationIndeces.values))]\n",
    "        testingIndeces = generateNIndecesFrom(750000,list(testingIndeces))\n",
    "        testingIndeces.to_csv('test_ids.txt',index=False,header=False)\n",
    "    \n",
    "    trainIndeces.columns = ['Index']\n",
    "    validationIndeces.columns = ['Index']\n",
    "    testingIndeces.columns = ['Index']\n",
    "\n",
    "    # Generate the actual data files\n",
    "    column_headers = getColumnHeaders()\n",
    "    train1M = pd.DataFrame()\n",
    "    train1M = generateSubSet(data_path,train1M,trainIndeces,4000000,46000000,column_headers)\n",
    "#     return train1M\n",
    "    generateSummaryStatsAndHists(train1M)\n",
    "    generateCategoricalData(train1M)\n",
    "    preProcessIntsAndSave(train1M,'scalerPickle.pkl')\n",
    "    \n",
    "    validation250k = pd.DataFrame()\n",
    "    validation250k = generateSubSet(data_path,validation250k,validationIndeces,4000000,46000000,column_headers)\n",
    "\n",
    "    test750k = pd.DataFrame()\n",
    "    test750k = generateSubSet(data_path,test750k,testingIndeces,4000000,46000000,column_headers)\n",
    "    \n",
    "    \n",
    "    print(train1M.shape)\n",
    "    print(validation250k.shape)\n",
    "    print(test750k.shape)\n",
    "\n",
    "    return train1M[train1M.columns[1:40]].values, train1M['label'].values, validation250k[validation250k.columns[1:40]].values, validation250k['label'].values, test750k[test750k.columns[1:40]].values, test750k['label'].values\n",
    "\n",
    "def preprocess_int_data(data, features):\n",
    "    n = len([f for f in features if f < 13])\n",
    "    \n",
    "    dataFrame = pd.DataFrame()\n",
    "    for f in features:\n",
    "        if f < 13:\n",
    "            dataFrame = pd.concat([dataFrame, pd.DataFrame(data[:,f:f+1])],axis=1)\n",
    "\n",
    "    headers = getDataHeaders()\n",
    "    trueHeaders = []\n",
    "    for f in features:\n",
    "        if f < 13:\n",
    "            trueHeaders.append(headers[f])\n",
    "    \n",
    "    dataFrame.columns = trueHeaders\n",
    "\n",
    "    for f in features:\n",
    "        if f < 13:\n",
    "\n",
    "            dataFrame[dataFrame.columns[f]] = dataFrame[dataFrame.columns[f]].fillna(0)\n",
    "            dataFrame[dataFrame.columns[f]] = dataFrame[dataFrame.columns[f]].replace(-1,0)\n",
    "            dataFrame[dataFrame.columns[f]] = dataFrame[dataFrame.columns[f]].replace(-2,0)\n",
    "\n",
    "    scaler = joblib.load('scalerPickle.pkl') \n",
    "    scaledValues = scaler.transform(dataFrame)\n",
    "    \n",
    "    return scaledValues\n",
    "\n",
    "\n",
    "def preprocess_cat_data(data, features, preprocess):\n",
    "#     print(features)\n",
    "#     print(preprocess)\n",
    "#     print(data)\n",
    "    dataFrame = pd.DataFrame(data)\n",
    "#     print(dataFrame[dataFrame.columns[13:39]])\n",
    "    # Change each column in the 13-39 into categorical\n",
    "#     dataFrame.columns = getDataHeaders()\n",
    "    \n",
    "    returnFrame = pd.DataFrame()\n",
    "    # drop the cols that are not in the features vector\n",
    "    for col in dataFrame.columns:\n",
    "        foundCol = False\n",
    "        for f in features:\n",
    "            if (f == col):\n",
    "                foundCol = True\n",
    "                \n",
    "        if foundCol == False:\n",
    "            dataFrame.drop(col,inplace=True,axis=1)\n",
    "        else:    \n",
    "            # I know that the categorical features start at 1 and index 13 so add 12 to f\n",
    "            if (col > 12):\n",
    "                print(col)\n",
    "#                 print(dataFrame[col].dtype)\n",
    "                dataFrame[col] = dataFrame[col].astype('category')\n",
    "                curFeatures = pd.read_csv(\"categorical_\" + str(col-12) + \"_features.csv\",header = None,index_col = 0)\n",
    "#                 print(curFeatures.values)\n",
    "                dataFrame[col].cat.set_categories(curFeatures.values)\n",
    "                dataFrame[col].cat.add_categories(new_categories = 'Dummy',inplace = True)\n",
    "                dataFrame[col] = dataFrame[col].fillna('Dummy')\n",
    "#                 print(dataFrame[col].dtype)\n",
    "# #                 print(dataFrame[col].cat.categories)\n",
    "                onehotVals = pd.get_dummies(dataFrame[col],prefix='encoded',sparse=True)\n",
    "#                 print(onehotVals.info())\n",
    "                returnFrame = pd.concat([returnFrame, onehotVals],axis=1)\n",
    "    \n",
    "                del onehotVals\n",
    "        \n",
    "                print(\"Got 1hot for \" + str(col))\n",
    "#                 print(returnFrame.info())\n",
    "#                 return\n",
    "    \n",
    "#     print(dataFrame.head())\n",
    "#     print(dataFrame.info())\n",
    "#     print(returnFrame.head())\n",
    "    \n",
    "#     for col in dataFrame.columns[13:39]:\n",
    "#         dataFrame[col] = dataFrame[col].astype('category')\n",
    "#         #reset the categories to the ones for that column\n",
    "#         \n",
    "        \n",
    "#         dataFrame[col].cat.set_categories(curFeatures.values)\n",
    "#         pd.get_dummies(train1M[col],prefix=['encoded'],sparse=True)\n",
    "#     dataFrame[dataFrame.columns[13:39]] = dataFrame[dataFrame.columns[13:39]].fillna('Dummy')\n",
    "    return dataFrame.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def check_output_features(fs):\n",
    "    assert len(fs) < 39\n",
    "\n",
    "\n",
    "def check_output_read_data(data, target, n):\n",
    "    assert data.shape[0] == n\n",
    "    assert target.shape[0] == n\n",
    "\n",
    "\n",
    "def check_output_preprocess(preprocess):\n",
    "    assert (preprocess == 'onehot') or (preprocess == 'rate') or (preprocess == 'tfidf')\n",
    "\n",
    "\n",
    "def check_output_preprocess_int_data(data, fs):\n",
    "    n = len([f for f in fs if f < 13])\n",
    "    assert data.shape[1] == n\n",
    "\n",
    "\n",
    "def check_output_preprocess_cat_data(data, fs, preprocess):\n",
    "    pass\n",
    "\n",
    "\n",
    "def read_features(path):\n",
    "    features = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            features.append(int(line.strip()))\n",
    "    return features\n",
    "\n",
    "\n",
    "def read_preprocess(path):\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            return line.strip()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = './dac/train.txt'\n",
    "train = 'train_ids.txt'\n",
    "validation = 'validation_ids.txt'\n",
    "test = 'test_ids.txt'\n",
    "features = 'features.txt'\n",
    "preprocess = 'preprocess.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train1M = read_data(data, train, validation, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generateCategoricalData(train1M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dac/train.txt\n",
      "train_ids.txt\n",
      "validation_ids.txt\n",
      "test_ids.txt\n",
      "Extraction Stats: 87443 percent: 8.7443%\n",
      "Extraction Stats: 174728 percent: 17.4728%\n",
      "Extraction Stats: 262233 percent: 26.2233%\n",
      "Extraction Stats: 349388 percent: 34.9388%\n",
      "Extraction Stats: 436039 percent: 43.6039%\n",
      "Extraction Stats: 523389 percent: 52.3389%\n",
      "Extraction Stats: 610528 percent: 61.0528%\n",
      "Extraction Stats: 698062 percent: 69.80619999999999%\n",
      "Extraction Stats: 785627 percent: 78.56269999999999%\n",
      "Extraction Stats: 872569 percent: 87.2569%\n",
      "Extraction Stats: 960224 percent: 96.02239999999999%\n",
      "Extraction Stats: 1000000 percent: 100.0%\n",
      "(34,)\n",
      "(87,)\n",
      "(22391,)\n",
      "(10501,)\n",
      "(9,)\n",
      "(3,)\n",
      "(2103,)\n",
      "(17,)\n",
      "(1,)\n",
      "(3411,)\n",
      "(964,)\n",
      "(22740,)\n",
      "(614,)\n",
      "(4,)\n",
      "(1296,)\n",
      "(16804,)\n",
      "(3,)\n",
      "(511,)\n",
      "(121,)\n",
      "(1,)\n",
      "(16323,)\n",
      "(2,)\n",
      "(4,)\n",
      "(3154,)\n",
      "(9,)\n",
      "(2776,)\n",
      "Extraction Stats: 21799 percent: 8.7196%\n",
      "Extraction Stats: 43529 percent: 17.4116%\n",
      "Extraction Stats: 65147 percent: 26.058799999999998%\n",
      "Extraction Stats: 86951 percent: 34.7804%\n",
      "Extraction Stats: 108874 percent: 43.5496%\n",
      "Extraction Stats: 130707 percent: 52.282799999999995%\n",
      "Extraction Stats: 152894 percent: 61.1576%\n",
      "Extraction Stats: 175020 percent: 70.00800000000001%\n",
      "Extraction Stats: 196711 percent: 78.6844%\n",
      "Extraction Stats: 218196 percent: 87.2784%\n",
      "Extraction Stats: 239920 percent: 95.968%\n",
      "Extraction Stats: 250000 percent: 100.0%\n",
      "Extraction Stats: 64888 percent: 8.651733333333333%\n",
      "Extraction Stats: 130570 percent: 17.409333333333333%\n",
      "Extraction Stats: 195925 percent: 26.12333333333333%\n",
      "Extraction Stats: 261418 percent: 34.85573333333333%\n",
      "Extraction Stats: 327041 percent: 43.605466666666665%\n",
      "Extraction Stats: 392404 percent: 52.32053333333333%\n",
      "Extraction Stats: 457908 percent: 61.0544%\n",
      "Extraction Stats: 523163 percent: 69.75506666666666%\n",
      "Extraction Stats: 588311 percent: 78.44146666666667%\n",
      "Extraction Stats: 653742 percent: 87.1656%\n",
      "Extraction Stats: 719587 percent: 95.94493333333334%\n",
      "Extraction Stats: 750000 percent: 100.0%\n",
      "(1000000, 41)\n",
      "(250000, 41)\n",
      "(750000, 41)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2e440b7e390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_data, train_target, validation_data, validation_target, test_data, test_target = \\\n",
    "    read_data(data, train, validation, test)\n",
    "\n",
    "check_output_read_data(train_data, train_target, 1000000)\n",
    "check_output_read_data(validation_data, validation_target, 250000)\n",
    "check_output_read_data(test_data, test_target, 750000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # backup\n",
    "# train_data2 = train_data\n",
    "# train_target2 = train_target\n",
    "# validation_data2 = validation_data\n",
    "# validation_target2 = validation_target\n",
    "# test_data2 = test_data\n",
    "# test_target2 = test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# del train_data2\n",
    "# del train_target2\n",
    "# del validation_data2\n",
    "# del validation_target2\n",
    "# del test_data2\n",
    "# del test_target2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(train_data, 'train_data.pkl')\n",
    "# joblib.dump(train_target, 'train_target.pkl')\n",
    "# joblib.dump(validation_data, 'validation_data.pkl')\n",
    "# joblib.dump(validation_target, 'validation_target.pkl')\n",
    "# joblib.dump(test_data, 'test_data.pkl')\n",
    "# joblib.dump(test_target, 'test_target.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #restore\n",
    "# train_data = train_data2\n",
    "# train_target = train_target2\n",
    "# validation_data = validation_data2\n",
    "# validation_target = validation_target2\n",
    "# test_data = test_data2\n",
    "# test_target = test_target2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_data = joblib.load('train_data.pkl')\n",
    "# train_target = joblib.load('train_target.pkl')\n",
    "# validation_data = joblib.load('validation_data.pkl')\n",
    "# validation_target = joblib.load('validation_target.pkl')\n",
    "# test_data = joblib.load('test_data.pkl')\n",
    "# test_target = joblib.load('test_target.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = read_features(features)\n",
    "\n",
    "check_output_features(features)\n",
    "\n",
    "preprocess = read_preprocess(preprocess)\n",
    "\n",
    "check_output_preprocess(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_int_data = preprocess_int_data(train_data, features)\n",
    "validation_int_data = preprocess_int_data(validation_data, features)\n",
    "test_int_data = preprocess_int_data(test_data, features)\n",
    "\n",
    "check_output_preprocess_int_data(train_int_data, features)\n",
    "check_output_preprocess_int_data(validation_int_data, features)\n",
    "check_output_preprocess_int_data(test_int_data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getsizeof(train_data)+getsizeof(train_target)+getsizeof(validation_data)+getsizeof(validation_target)+getsizeof(test_data)+getsizeof(test_target)+getsizeof(train_int_data)+getsizeof(validation_int_data)+getsizeof(test_int_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "Got 1hot for 14\n",
      "18\n",
      "Got 1hot for 18\n",
      "19\n",
      "Got 1hot for 19\n",
      "21\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-e86efc9df28d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_cat_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_cat_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvalidation_cat_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_cat_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_cat_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_cat_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcheck_output_preprocess_cat_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_cat_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-18b80480d574>\u001b[0m in \u001b[0;36mpreprocess_cat_data\u001b[1;34m(data, features, preprocess)\u001b[0m\n\u001b[0;32m    242\u001b[0m                 \u001b[0monehotVals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataFrame\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'encoded'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;31m#                 print(onehotVals.info())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m                 \u001b[0mreturnFrame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mreturnFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monehotVals\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0monehotVals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adriencogny\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[0;32m    204\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m                        copy=copy)\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adriencogny\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[1;31m# consolidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m             \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m             \u001b[0mndims\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adriencogny\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(self, inplace)\u001b[0m\n\u001b[0;32m   3043\u001b[0m         \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inplace'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3044\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3045\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3046\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3047\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adriencogny\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3025\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3027\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3029\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adriencogny\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   3014\u001b[0m         \"\"\"\n\u001b[0;32m   3015\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3016\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3017\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3018\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adriencogny\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mf\u001b[1;34m()\u001b[0m\n\u001b[0;32m   3023\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3024\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3025\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3027\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adriencogny\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconsolidate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3571\u001b[0m         \u001b[0mbm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3572\u001b[0m         \u001b[0mbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3573\u001b[1;33m         \u001b[0mbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3574\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adriencogny\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3577\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3578\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3579\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3580\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adriencogny\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   4523\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4524\u001b[0m         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n\u001b[1;32m-> 4525\u001b[1;33m                                       _can_consolidate=_can_consolidate)\n\u001b[0m\u001b[0;32m   4526\u001b[0m         \u001b[0mnew_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4527\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adriencogny\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, _can_consolidate)\u001b[0m\n\u001b[0;32m   4543\u001b[0m         \u001b[1;31m# combination of those slices is a slice, too.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4544\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_array\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4545\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_vstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4547\u001b[0m         \u001b[0margsort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adriencogny\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_vstack\u001b[1;34m(to_stack, dtype)\u001b[0m\n\u001b[0;32m   4589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4590\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4591\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_stack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adriencogny\\Anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \"\"\"\n\u001b[1;32m--> 234\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_cat_data = preprocess_cat_data(train_data, features, preprocess)\n",
    "validation_cat_data = preprocess_cat_data(validation_data, features, preprocess)\n",
    "test_cat_data = preprocess_cat_data(test_data, features, preprocess)\n",
    "\n",
    "check_output_preprocess_cat_data(train_cat_data, features, preprocess)\n",
    "check_output_preprocess_cat_data(validation_cat_data, features, preprocess)\n",
    "check_output_preprocess_cat_data(test_cat_data, features, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
