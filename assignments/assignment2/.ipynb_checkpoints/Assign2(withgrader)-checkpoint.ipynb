{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional helper functions required for the code to run:\n",
    "# This function defines the column headers for the data frames\n",
    "def getColumnHeaders():\n",
    "    return pd.Series(data=['Index','label','integer_1','integer_2','integer_3',\n",
    "                                 'integer_4','integer_5','integer_6','integer_7','integer_8','integer_9',\n",
    "                                 'integer_10','integer_11','integer_12','integer_13','categorical_1',\n",
    "                                 'categorical_2','categorical_3','categorical_4','categorical_5','categorical_6',\n",
    "                                 'categorical_7','categorical_8','categorical_9','categorical_10','categorical_11',\n",
    "                                 'categorical_12','categorical_13','categorical_14','categorical_15','categorical_16',\n",
    "                                 'categorical_17','categorical_18','categorical_19','categorical_20','categorical_21',\n",
    "                                 'categorical_22','categorical_23','categorical_24','categorical_25','categorical_26'])\n",
    "\n",
    "# this function generateNIndecesFrom takes a range of Indeces and randomly takes n of them, returns a pandas Series object\n",
    "def generateNIndecesFrom(n, rangeOfIndeces):\n",
    "    # print(\"Generating \" + str(n) + \" indeces from range\")\n",
    "    allIndeces = random.sample(rangeOfIndeces, n)\n",
    "    allIndeces = pd.Series(data = allIndeces)\n",
    "    allIndeces = allIndeces.sort_values().reset_index().drop(['index'],axis=1)\n",
    "    allIndeces.columns = ['Index'];\n",
    "    return allIndeces\n",
    "\n",
    "# the generateSubSet function takes in a file, a dataFrame to put the data in as well as index values which should be extracted,\n",
    "# a number of rows per itteration (this is used to not overload the memory) , total number of rows in the file, the column headers for the new dataframe\n",
    "def generateSubSet(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers):\n",
    "    totalNumIterations = int(totalNumRows/numRowsPerItteration)\n",
    "    # print(\"Number of itterations = \" + str(totalNumIterations))\n",
    "    totalNumRowsTraversed = 0\n",
    "    prevsize = 0\n",
    "    for i in range(totalNumIterations + 1):\n",
    "#         \n",
    "#         print(\"Itteration number: \" + str(i))\n",
    "#         print(\"skipRows: \" + str(i * numRowsPerItteration))\n",
    "#         print(\"Read in : \" + str(numRowsPerItteration))\n",
    "        curData = pd.read_table(file,skiprows = i * numRowsPerItteration, nrows = numRowsPerItteration,header=None)\n",
    "        curData.index = [i for i in range(i*numRowsPerItteration,i*numRowsPerItteration + curData.shape[0])]\n",
    "        totalNumRowsTraversed = totalNumRowsTraversed + curData.shape[0]\n",
    "\n",
    "#         print(curData.shape)\n",
    "#         print(curData.index.shape)\n",
    "\n",
    "        curData['Index'] = curData.index\n",
    "        curData.columns = column_headers\n",
    "        \n",
    "        curIndexRange = indexValues['Index'][(indexValues['Index'] < (i*numRowsPerItteration + numRowsPerItteration)) & (indexValues['Index'] > (i*numRowsPerItteration-1))]\n",
    "        curData = curData[curData['Index'].isin(curIndexRange)]\n",
    "\n",
    "        dataFrame = pd.concat([dataFrame,curData])\n",
    "        \n",
    "        # clear_output()\n",
    "        # print(\"Extraction Stats: \" + str(dataFrame.shape[0]) + \" percent: \" + str(dataFrame.shape[0] / indexValues.shape[0] * 100) + \"%\")\n",
    "        # print(\"Document Stats: \" + str(totalNumRowsTraversed) + \" percent: \" + str(totalNumRowsTraversed/totalNumRows*100) + \"%\")\n",
    "        if (dataFrame.shape[0] - prevsize) > 500000:\n",
    "            prevsize = dataFrame.shape[0]\n",
    "#             dataFrame.to_csv(frameSaveName)\n",
    "#         elif dataFrame.shape[0] == indexValues.shape[0]:\n",
    "#             print(\"Finished with the data collection\")\n",
    "# #             dataFrame.to_csv(frameSaveName)\n",
    "#             break\n",
    "    # print(\"Extraction is Done, now saving frame\")        \n",
    "    return dataFrame\n",
    "\n",
    "# This method generates is a wrapper around the generateSubset to generate the subset and save the dataframe to a csv file (for being able to make use of it after)\n",
    "def generateAndSaveSubset(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers,frameSaveName):\n",
    "    dataFrame = generateSubSet(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers)\n",
    "    dataFrame.to_csv(frameSaveName)\n",
    "    return dataFrame\n",
    "\n",
    "def read_data(data_path, train_path, validation_path, test_path):\n",
    "\n",
    "    print(data_path)\n",
    "    print(train_path)\n",
    "    print(validation_path)\n",
    "    print(test_path)\n",
    "    \n",
    "    #get the ids\n",
    "    try:\n",
    "        trainIndeces = pd.read_csv(train_path,squeeze = True)\n",
    "        validationIndeces = pd.read_csv(validation_path,squeeze = True)\n",
    "        testingIndeces = pd.read_csv(test_path,squeeze = True)\n",
    "    except:\n",
    "        print(\"There were not 1000000 data points\")\n",
    "        trainIndeces = generateNIndecesFrom(1000000,list(twoMIndeces['Index']))\n",
    "        trainIndeces.to_csv('train_ids.txt',index=False,header=False)\n",
    "\n",
    "        remainingIndeces = twoMIndeces['Index'][~twoMIndeces['Index'].isin(trainIndeces.values)]\n",
    "        validationIndeces = generateNIndecesFrom(250000,list(remainingIndeces))\n",
    "        validationIndeces.to_csv('validation_ids.txt',index=False,header=False)\n",
    "\n",
    "        testingIndeces = twoMIndeces['Index'][~(twoMIndeces['Index'].isin(trainIndeces.values) | twoMIndeces['Index'].isin(validationIndeces.values))]\n",
    "        testingIndeces = generateNIndecesFrom(750000,list(testingIndeces))\n",
    "        testingIndeces.to_csv('test_ids.txt',index=False,header=False)\n",
    "    \n",
    "    print(trainIndeces.head())\n",
    "    print(validationIndeces.head())\n",
    "    print(testingIndeces.head())\n",
    "    \n",
    "    # Generate the actual data files\n",
    "    column_headers = getColumnHeaders()\n",
    "    # print(\"No 1M collection\")\n",
    "    train1M = pd.DataFrame()\n",
    "    train1M = generateSubSet(data_path,train1M,trainIndeces,4000000,46000000,column_headers)\n",
    "\n",
    "\n",
    "    # print(\"No 250k collection\")\n",
    "    validation250k = pd.DataFrame()\n",
    "    validation250k = generateAndSaveSubset(data_path,validation250k,validationIndeces,4000000,46000000,column_headers)\n",
    "\n",
    "\n",
    "    # print(\"No 750k collection\")\n",
    "    test750k = pd.DataFrame()\n",
    "    test750k = generateAndSaveSubset(data_path,test750k,validationIndeces,4000000,46000000,column_headers)\n",
    "\n",
    "    # train_data, train_target = np.zeros((1000000, 39)), np.zeros((1000000,))\n",
    "    # validation_data, validation_target = np.zeros((250000, 39)), np.zeros((250000,))\n",
    "    # test_data, test_target = np.zeros((750000, 39)), np.zeros((750000,))\n",
    "\n",
    "\n",
    "    # return train1M.to_array(), trainIndeces.to_array(), validation250k.to_array(), validation_target, test_data, test_target\n",
    "    return\n",
    "\n",
    "def preprocess_int_data(data, features):\n",
    "    n = len([f for f in features if f < 13])\n",
    "    return np.zeros((data.shape[0], n))\n",
    "\n",
    "\n",
    "def preprocess_cat_data(data, features, preprocess):\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dac/train.txt\n",
      "train_ids.txt\n",
      "validation_ids.txt\n",
      "test_ids.txt\n",
      "0    135\n",
      "1    177\n",
      "2    250\n",
      "3    329\n",
      "4    425\n",
      "Name: 19, dtype: int64\n",
      "0     716\n",
      "1     773\n",
      "2     986\n",
      "3     997\n",
      "4    1073\n",
      "Name: 601, dtype: int64\n",
      "0     86\n",
      "1    108\n",
      "2    116\n",
      "3    195\n",
      "4    215\n",
      "Name: 38, dtype: int64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-7e58b2bdc5a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mpreprocess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'preprocess.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_target\u001b[0m \u001b[1;33m=\u001b[0m         \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mcheck_output_read_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-fc93c007b466>\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(data_path, train_path, validation_path, test_path)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;31m# print(\"No 1M collection\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[0mtrain1M\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m     \u001b[0mtrain1M\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerateSubSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain1M\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainIndeces\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4000000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m46000000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumn_headers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-fc93c007b466>\u001b[0m in \u001b[0;36mgenerateSubSet\u001b[1;34m(file, dataFrame, indexValues, numRowsPerItteration, totalNumRows, column_headers)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m#         print(\"skipRows: \" + str(i * numRowsPerItteration))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m#         print(\"Read in : \" + str(numRowsPerItteration))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mcurData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mskiprows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnumRowsPerItteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumRowsPerItteration\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mcurData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnumRowsPerItteration\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnumRowsPerItteration\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcurData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mtotalNumRowsTraversed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotalNumRowsTraversed\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcurData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adriencogny\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adriencogny\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adriencogny\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    980\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 982\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adriencogny\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1717\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1718\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1719\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1720\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1721\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read (pandas\\_libs\\parsers.c:10862)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory (pandas\\_libs\\parsers.c:11563)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._concatenate_chunks (pandas\\_libs\\parsers.c:29188)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adriencogny\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    500\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0marr_or_dtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def check_output_features(fs):\n",
    "    assert len(fs) < 39\n",
    "\n",
    "\n",
    "def check_output_read_data(data, target, n):\n",
    "    assert data.shape[0] == n\n",
    "    assert target.shape[0] == n\n",
    "\n",
    "\n",
    "def check_output_preprocess(preprocess):\n",
    "    assert (preprocess == 'onehot') or (preprocess == 'rate') or (preprocess == 'tfidf')\n",
    "\n",
    "\n",
    "def check_output_preprocess_int_data(data, fs):\n",
    "    n = len([f for f in fs if f < 13])\n",
    "    assert data.shape[1] == n\n",
    "\n",
    "\n",
    "def check_output_preprocess_cat_data(data, fs, preprocess):\n",
    "    pass\n",
    "\n",
    "\n",
    "def read_features(path):\n",
    "    features = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            features.append(int(line.strip()))\n",
    "    return features\n",
    "\n",
    "\n",
    "def read_preprocess(path):\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            return line.strip()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = './dac/train.txt'\n",
    "    train = 'train_ids.txt'\n",
    "    validation = 'validation_ids.txt'\n",
    "    test = 'test_ids.txt'\n",
    "    features = 'features.txt'\n",
    "    preprocess = 'preprocess.txt'\n",
    "\n",
    "    train_data, train_target, validation_data, validation_target, test_data, test_target = \\\n",
    "        read_data(data, train, validation, test)\n",
    "\n",
    "    check_output_read_data(train_data, train_target, 1000000)\n",
    "    check_output_read_data(validation_data, validation_target, 250000)\n",
    "    check_output_read_data(test_data, test_target, 750000)\n",
    "\n",
    "    features = read_features(options.features)\n",
    "\n",
    "    check_output_features(features)\n",
    "\n",
    "    preprocess = read_preprocess(options.preprocess)\n",
    "\n",
    "    check_output_preprocess(preprocess)\n",
    "\n",
    "    train_int_data = preprocess_int_data(train_data, features)\n",
    "    validation_int_data = preprocess_int_data(validation_data, features)\n",
    "    test_int_data = preprocess_int_data(test_data, features)\n",
    "\n",
    "    check_output_preprocess_int_data(train_int_data, features)\n",
    "    check_output_preprocess_int_data(validation_int_data, features)\n",
    "    check_output_preprocess_int_data(test_int_data, features)\n",
    "\n",
    "    train_cat_data = preprocess_cat_data(train_data, features, preprocess)\n",
    "    validation_cat_data = preprocess_cat_data(validation_data, features, preprocess)\n",
    "    test_cat_data = preprocess_cat_data(test_data, features, preprocess)\n",
    "\n",
    "    check_output_preprocess_cat_data(train_cat_data, features, preprocess)\n",
    "    check_output_preprocess_cat_data(validation_cat_data, features, preprocess)\n",
    "    check_output_preprocess_cat_data(test_cat_data, features, preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
