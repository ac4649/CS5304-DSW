{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code for assign4.py\n",
    "\n",
    "\n",
    "# load_sst can be used to read the files from sst, which can be downloaded from this link:\n",
    "\n",
    "\n",
    "#   https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
    "\n",
    "  \n",
    "\n",
    "# load_embeddings can be used to read files in the text format. Here's a link to\n",
    "\n",
    "\n",
    "\n",
    "#   word2vec - https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "\n",
    "#   GloVe (300D 6B) - http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "\n",
    "\n",
    "\n",
    "# The word2vec file is saved in a binary format and will need to be converted to text format. This can be done by installing gensim:\n",
    "\n",
    "#   pip install --upgrade gensim  \n",
    "\n",
    "# Then running this snippet:\n",
    "\n",
    "\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# model.save_word2vec_format('GoogleNews-vectors-negative300.txt', binary=False)\n",
    "\n",
    "\n",
    "\n",
    "# To train:\n",
    "\n",
    "#   python assign4.py\n",
    "\n",
    "# To write test predictions:\n",
    "\n",
    "#   python assign4.py --eval_only_mode\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "PAD_TOKEN = '_PAD_'\n",
    "UNK_TOKEN = '_UNK_'\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "gpus = [0]\n",
    "torch.cuda.set_device(gpus[0])\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor\n",
    "mydir = os.path.dirname(os.path.abspath(\"assign4\"))\n",
    "\n",
    "\n",
    "# Methods for loading SST data\n",
    "\n",
    "def sentiment2label(x):\n",
    "    if x >= 0 and x <= 0.2:\n",
    "        return 0\n",
    "    elif x > 0.2 and x <= 0.4:\n",
    "        return 1\n",
    "    elif x > 0.4 and x <= 0.6:\n",
    "        return 2\n",
    "    elif x > 0.6 and x <= 0.8:\n",
    "        return 3\n",
    "    elif x > 0.8 and x <= 1:\n",
    "        return 4\n",
    "    else:\n",
    "        raise ValueError('Improper sentiment value {}'.format(x))\n",
    "\n",
    "def read_dictionary_txt_with_phrase_ids(dictionary_path, phrase_ids_path, labels_path=None):\n",
    "    print('Reading data dictionary_path={} phrase_ids_path={} labels_path={}'.format(\n",
    "        dictionary_path, phrase_ids_path, labels_path))\n",
    "\n",
    "    with open(phrase_ids_path) as f:\n",
    "        phrase_ids = set(line.strip() for line in f)\n",
    "\n",
    "    with open(dictionary_path) as f:\n",
    "        examples_dict = dict()\n",
    "        for line in f:\n",
    "            parts = line.strip().split('|')\n",
    "            phrase = parts[0]\n",
    "            phrase_id = parts[1]\n",
    "\n",
    "            if phrase_id not in phrase_ids:\n",
    "                continue\n",
    "\n",
    "            example = dict()\n",
    "            example['phrase'] = phrase.replace('(', '-LRB').replace(')', '-RRB-')\n",
    "            example['tokens'] = example['phrase'].split(' ')\n",
    "            example['example_id'] = phrase_id\n",
    "            example['label'] = None\n",
    "            examples_dict[example['example_id']] = example\n",
    "\n",
    "    if labels_path is not None:\n",
    "        with open(labels_path) as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                parts = line.strip().split('|')\n",
    "                phrase_id = parts[0]\n",
    "                sentiment = float(parts[1])\n",
    "                label = sentiment2label(sentiment)\n",
    "\n",
    "                if phrase_id in examples_dict:\n",
    "                    examples_dict[phrase_id]['label'] = label\n",
    "\n",
    "    examples = [ex for _, ex in examples_dict.items()]\n",
    "\n",
    "    print('Found {} examples.'.format(len(examples)))\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def build_vocab(datasets):\n",
    "    vocab = dict()\n",
    "    vocab[PAD_TOKEN] = len(vocab)\n",
    "    vocab[UNK_TOKEN] = len(vocab)\n",
    "    for data in datasets:\n",
    "        for example in data:\n",
    "            for word in example['tokens']:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = len(vocab)\n",
    "\n",
    "    print('Vocab size: {}'.format(len(vocab)))\n",
    "\n",
    "    return vocab\n",
    "\n",
    "class TokenConverter(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.unknown = 0\n",
    "\n",
    "    def convert(self, token):\n",
    "        if token in self.vocab:\n",
    "            id = self.vocab.get(token)\n",
    "        else:\n",
    "            id = self.vocab.get(UNK_TOKEN)\n",
    "            self.unknown += 1\n",
    "        return id\n",
    "\n",
    "\n",
    "def convert2ids(data, vocab):\n",
    "    converter = TokenConverter(vocab)\n",
    "    for example in data:\n",
    "        example['tokens'] = list(map(converter.convert, example['tokens']))\n",
    "    print('Found {} unknown tokens.'.format(converter.unknown))\n",
    "    return data\n",
    "\n",
    "def concatEmbeddings(vocab1, e1, vocab2, e2):\n",
    "    \n",
    "    superVocab = set(vocab1.keys()) & set(vocab2.keys())\n",
    "    superEmbed = np.ndarray(shape=(len(superVocab), (len(e1[0])+len(e2[0]))))\n",
    "    i=0\n",
    "    for word in superVocab:\n",
    "        id1 = vocab1[word]\n",
    "        id2 = vocab2[word]\n",
    "        superEmbed[i]= np.concatenate((e1[id1], e2[id2]))\n",
    "        i+=1\n",
    "        #print(id1)\n",
    "        #print(id2)\n",
    "        \n",
    "        #np.append(superEmbed,np.concatenate((e1[id1], e2[id2])))\n",
    "    print (superEmbed.shape)    \n",
    "    returnedDict = dict((word,index) for word in superVocab for index in range(len(superVocab)))\n",
    "    \n",
    "    return returnedDict, superEmbed\n",
    "\n",
    "def load_data_and_embeddings(data_path, phrase_ids_path, embeddings_path):\n",
    "    labels_path = os.path.join(data_path, 'sentiment_labels.txt')\n",
    "    dictionary_path = os.path.join(data_path, 'dictionary.txt')\n",
    "    train_data = read_dictionary_txt_with_phrase_ids(dictionary_path, os.path.join(phrase_ids_path, 'phrase_ids.train.txt'), labels_path)\n",
    "    validation_data = read_dictionary_txt_with_phrase_ids(dictionary_path, os.path.join(phrase_ids_path, 'phrase_ids.dev.txt'), labels_path)\n",
    "    test_data = read_dictionary_txt_with_phrase_ids(dictionary_path, os.path.join(phrase_ids_path, 'phrase_ids.test.txt'))\n",
    "    vocab = build_vocab([train_data, validation_data, test_data])\n",
    "    \n",
    "    if options.use_embeddings == 0:  \n",
    "        vocab, embeddings = load_embeddings(options.embeddings, vocab, cache=True)\n",
    "    elif options.use_embeddings == 1:\n",
    "        vocab, embeddings = load_embeddings(options.embeddings2, vocab, cache=True)\n",
    "    elif options.use_embeddings == 2:\n",
    "        vocab1, embeddings1 = load_embeddings(options.embeddings, vocab, cache=True)\n",
    "        vocab2, embeddings2 = load_embeddings(options.embeddings2, vocab, cache=True)\n",
    "    \n",
    "        print(embeddings1.shape)\n",
    "        print(embeddings2.shape)\n",
    "        \n",
    "        vocab, embeddings = concatEmbeddings(vocab1, embeddings1, vocab2, embeddings2)\n",
    "        \n",
    "        \n",
    "    train_data = convert2ids(train_data, vocab)\n",
    "    validation_data = convert2ids(validation_data, vocab)\n",
    "    test_data = convert2ids(test_data, vocab)\n",
    "    return train_data, validation_data, test_data, vocab, embeddings\n",
    "\n",
    "\n",
    "def load_embeddings(path, vocab, cache=False, cache_path=None):\n",
    "    rows = []\n",
    "    new_vocab = [UNK_TOKEN]\n",
    "\n",
    "    if cache_path is None:\n",
    "        cache_path = path + '.cache'\n",
    "\n",
    "  # Use cache file if it exists.\n",
    "    if os.path.exists(cache_path):\n",
    "        path = cache_path\n",
    "\n",
    "    print(\"Reading embeddings from {}\".format(path))\n",
    "\n",
    "  # first pass over the embeddings to vocab and relevant rows\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, row = line.split(' ', 1)\n",
    "            if word == UNK_TOKEN:\n",
    "                raise ValueError('The unk token should not exist w.in embeddings.')\n",
    "            if word in vocab:\n",
    "                rows.append(line)\n",
    "                new_vocab.append(word)\n",
    "\n",
    "  # optionally save relevant rows to cache file.\n",
    "    if cache and not os.path.exists(cache_path):\n",
    "        with open(cache_path, 'w') as f:\n",
    "            for line in rows:\n",
    "                f.write(line)\n",
    "            print(\"Cached embeddings to {}\".format(cache_path))\n",
    "\n",
    "  # turn vocab list into a dictionary\n",
    "    new_vocab = {w: i for i, w in enumerate(new_vocab)}\n",
    "\n",
    "    print('New vocab size: {}'.format(len(new_vocab)))\n",
    "\n",
    "    assert len(rows) == len(new_vocab) - 1\n",
    "\n",
    "  # create embeddings matrix\n",
    "    embeddings = np.zeros((len(new_vocab), 300), dtype=np.float32)\n",
    "    for i, line in enumerate(rows):\n",
    "        embeddings[i+1] = list(map(float, line.strip().split(' ')[1:]))\n",
    "\n",
    "    return new_vocab, embeddings\n",
    "\n",
    "# Batch Iterator\n",
    "\n",
    "def prepare_data(data):\n",
    "  # pad data\n",
    "    maxlen = max(map(len, data))\n",
    "    data = [ex + [0] * (maxlen-len(ex)) for ex in data]\n",
    "\n",
    "  # wrap in tensor\n",
    "    return torch.LongTensor(data)\n",
    "\n",
    "\n",
    "def prepare_labels(labels):\n",
    "    try:\n",
    "        return torch.LongTensor(labels)\n",
    "    except:\n",
    "        return labels\n",
    "\n",
    "\n",
    "def batch_iterator(dataset, batch_size, forever=False):\n",
    "    dataset_size = len(dataset)\n",
    "    order = None\n",
    "    nbatches = dataset_size // batch_size\n",
    "\n",
    "    def init_order():\n",
    "        return random.sample(range(dataset_size), dataset_size)\n",
    "\n",
    "    def get_batch(start, end):\n",
    "        batch = [dataset[ii] for ii in order[start:end]]\n",
    "        data = prepare_data([ex['tokens'] for ex in batch])\n",
    "        labels = prepare_labels([ex['label'] for ex in batch])\n",
    "        example_ids = [ex['example_id'] for ex in batch]\n",
    "        return data, labels, example_ids\n",
    "\n",
    "    def pad_to_batch(batch):\n",
    "        x,y = zip(*batch)\n",
    "        max_x = max([s.size(1) for s in x])\n",
    "        x_p = []\n",
    "        for i in range(len(batch)):\n",
    "            if x[i].size(1) < max_x:\n",
    "                x_p.append(torch.cat([x[i], Variable(LongTensor([word2index['<PAD>']] * (max_x - x[i].size(1)))).view(1, -1)], 1))\n",
    "            else:\n",
    "                x_p.append(x[i])\n",
    "        return torch.cat(x_p), torch.cat(y).view(-1)\n",
    "\n",
    "    order = init_order()\n",
    "\n",
    "    while True:\n",
    "        for i in range(nbatches):\n",
    "            start = i*batch_size\n",
    "            end = (i+1)*batch_size\n",
    "            yield get_batch(start, end)\n",
    "\n",
    "\n",
    "        if nbatches*batch_size < dataset_size:\n",
    "            yield get_batch(nbatches*batch_size, dataset_size)\n",
    "\n",
    "        if not forever:\n",
    "            break\n",
    "    \n",
    "        order = init_order()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "class  CNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, kernel_dim=100, kernel_sizes=(3, 4, 5), dropout=0.5):\n",
    "        super(CNNClassifier,self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, kernel_dim, (K, embedding_dim)) for K in kernel_sizes])\n",
    "\n",
    "        # kernal_size = (K,D) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * kernel_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def init_weights(self, pretrained_word_vectors, is_static=False):\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(pretrained_word_vectors).float())\n",
    "        if is_static:\n",
    "            self.embedding.weight.requires_grad = False ### CHANGEEEDD THIS \n",
    "\n",
    "\n",
    "    def forward(self, inputs, is_training=False):\n",
    "        inputs = self.embedding(inputs).unsqueeze(1) # (B,1,T,D)\n",
    "        inputs = [F.relu(conv(inputs)).squeeze(3) for conv in self.convs] #[(N,Co,W), ...]*len(Ks)\n",
    "        inputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in inputs] #[(N,Co), ...]*len(Ks)\n",
    "\n",
    "        concated = torch.cat(inputs, 1)\n",
    "\n",
    "        if is_training:\n",
    "            concated = self.dropout(concated) # (N,len(Ks)*Co)\n",
    "        out = self.fc(concated) \n",
    "        return F.log_softmax(out,1)\n",
    "\n",
    "\n",
    "\n",
    "# class BagOfWordsModel(nn.Module):\n",
    "#     def __init__(self, embeddings):\n",
    "#         super(BagOfWordsModel, self).__init__()\n",
    "#         self.embed = nn.Embedding(embeddings.shape[0], embeddings.shape[1], sparse=True)\n",
    "#         self.embed.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "#         self.classify = nn.Linear(embeddings.shape[1], 5)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.classify(self.embed(x).sum(1))\n",
    "\n",
    "\n",
    "# Utility Methods\n",
    "\n",
    "def checkpoint_model(step, val_err, model, opt, save_path):\n",
    "    save_dict = dict(\n",
    "        step=step,\n",
    "        val_err=val_err,\n",
    "        model_state_dict=model.state_dict(),\n",
    "        opt_state_dict=opt.state_dict())\n",
    "    torch.save(save_dict, save_path)\n",
    "\n",
    "\n",
    "def load_model(model, opt, load_path):\n",
    "    load_dict = torch.load(load_path)\n",
    "    step = load_dict['step']\n",
    "    val_err = load_dict['val_err']\n",
    "    model.load_state_dict(load_dict['model_state_dict'])\n",
    "    opt.load_state_dict(load_dict['opt_state_dict'])\n",
    "    return step, val_err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "\n",
    "def run_validation(model, dataset, options):\n",
    "    err = 0\n",
    "    count = 0\n",
    "    for data, labels, _ in batch_iterator(dataset, options.batch_size, forever=False):\n",
    "        outp = model(Variable(data))\n",
    "        loss = nn.NLLLoss()(F.log_softmax(outp), Variable(labels))\n",
    "        acc = (outp.data.max(1)[1] == labels).sum() / data.shape[0]\n",
    "        err += (1-acc) * data.shape[0]\n",
    "        count += data.shape[0]\n",
    "    err = err / count\n",
    "    print('Ev-Err={}'.format(err))\n",
    "    return err\n",
    "\n",
    "\n",
    "def run_test(model, dataset, options):\n",
    "    print('Writing predictions to {}'.format(os.path.abspath(options.predictions)))\n",
    "\n",
    "    preds_dict = dict()\n",
    "\n",
    "    for data, _, example_ids in batch_iterator(dataset, options.batch_size, forever=False):\n",
    "        outp = model(Variable(data))\n",
    "        preds = outp.data.max(1)[1]\n",
    "\n",
    "        for id, pred in zip(example_ids, preds):\n",
    "            preds_dict[id] = pred\n",
    "\n",
    "    with open(options.predictions, 'w') as f:\n",
    "        for id, pred in preds_dict.items():\n",
    "            f.write('{}|{}\\n'.format(id, pred))\n",
    "\n",
    "\n",
    "def run(options):\n",
    "    \n",
    "    train_data, validation_data, test_data, vocab, embeddings = \\\n",
    "    load_data_and_embeddings(options.data, options.ids, options.embeddings)       \n",
    "        \n",
    "    EPOCH = 5\n",
    "    BATCH_SIZE = 50\n",
    "    KERNEL_SIZES = [3,4,5]\n",
    "    KERNEL_DIM = 100\n",
    "    LR = 0.001\n",
    "    \n",
    "#     if options.embeddings\n",
    "#     if options.use_embeddings == 0:  \n",
    "#         vocab, embeddings = load_embeddings(options.embeddings, vocab, cache=True)\n",
    "    model = CNNClassifier(len(vocab), embeddings.shape[1], 5, KERNEL_DIM, KERNEL_SIZES)\n",
    "    model.init_weights(embeddings) # initialize embedding matrix using pretrained vectors\n",
    "\n",
    "    USE_CUDA=True\n",
    "    if USE_CUDA:\n",
    "        model = model.cuda()\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    inputs_check = []\n",
    "    for epoch in range(3):\n",
    "        losses = []\n",
    "    #     for i,batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "        for i, (data, labels, _) in enumerate(batch_iterator(train_data, options.batch_size, forever=False)):\n",
    "    #         inputs,targets = pad_to_batch(batch)\n",
    "\n",
    "            inputs = Variable(data).cuda()\n",
    "            targets = Variable(labels).cuda()\n",
    "            inputs_check.append(inputs)\n",
    "\n",
    "            model.zero_grad()\n",
    "            preds = model(inputs, True)\n",
    "            \n",
    "            loss = loss_function(preds, targets)\n",
    "\n",
    "\n",
    "            losses.append(loss.data.tolist()[0])\n",
    "            loss.backward()\n",
    "\n",
    "                    #for param in model.parameters():\n",
    "                    #    param.grad.data.clamp_(-3, 3)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 500 == 0:\n",
    "                acc = (preds.data.max(1)[1] == labels.cuda()).sum() / preds.shape[0]\n",
    "#                 print (preds.data.max(1)[1])\n",
    "                print(\"Mean Loss : %0.2f, Acc: %0.2f\" %(np.mean(losses), acc))\n",
    "                # calculate validation accuracy at the end of each epoch\n",
    "                losses = []\n",
    "\n",
    "    def get_accuracy(model,dataset,batch_size):\n",
    "        validation_accuracy = []\n",
    "        for i, (data, labels, _) in enumerate(batch_iterator(dataset, batch_size, forever=False)):\n",
    "            inputs = Variable(data).cuda()\n",
    "            targets = Variable(labels).cuda()\n",
    "            preds = model(inputs, True)\n",
    "            acc = (preds.data.max(1)[1] == labels.cuda()).sum() / preds.shape[0]\n",
    "            validation_accuracy.append(acc)\n",
    "        return np.mean(np.array(validation_accuracy))\n",
    "    \n",
    "    def get_test(model,dataset,batch_size):\n",
    "        validation_accuracy = []\n",
    "        predictions = dict()\n",
    "        \n",
    "        with open(options.predictions, 'w'):\n",
    "            print(\"removing file\")\n",
    "            \n",
    "        for i, (data, _, example_ids) in enumerate(batch_iterator(dataset, batch_size, forever=False)):\n",
    "#             print(data.shape)\n",
    "            inputs = Variable(data).cuda()\n",
    "            preds = model(inputs, True)\n",
    "            preds = preds.data.max(1)[1]\n",
    "            \n",
    "            for id, pred in zip(example_ids, preds):\n",
    "                predictions[id] = pred\n",
    "            \n",
    "#             for x in np.array(preds.data.max(1)[1]):\n",
    "#                 predictions.append(x)\n",
    "\n",
    "            with open(options.predictions, 'a') as f:\n",
    "                for id, pred in predictions.items():\n",
    "                    f.write('{}|{}\\n'.format(id, pred))\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "#      preds_dict = dict()\n",
    "\n",
    "#     for data, _, example_ids in batch_iterator(dataset, options.batch_size, forever=False):\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "#     print(\"Validation accuracy\", get_accuracy(model,validation_data,options.batch_size))\n",
    "    test_labels = get_test(model,test_data,options.batch_size)\n",
    "    print (len(test_labels))\n",
    "    \n",
    "    f = open(\"results.txt\", \"w\")\n",
    "    f.write(\"\\n\".join(map(lambda x: str(x), test_labels)))\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"batch_size\": 32,\n",
      "    \"data\": \"~\\\\Desktop\\\\DataScience\\\\assign4\",\n",
      "    \"embeddings\": \"~\\\\Desktop\\\\DataScience\\\\assign4\\\\GoogleNews-vectors-negative300.txt\",\n",
      "    \"embeddings2\": \"~\\\\Desktop\\\\DataScience\\\\assign4\\\\glove.840B.300d.txt\",\n",
      "    \"eval_every\": 1000,\n",
      "    \"eval_only_mode\": false,\n",
      "    \"ids\": \"/home/adrien/Documents/CS5304-DSW/assignments/assignment4\",\n",
      "    \"log_every\": 100,\n",
      "    \"model\": \"/home/adrien/Documents/CS5304-DSW/assignments/assignment4/model.ckpt\",\n",
      "    \"predictions\": \"/home/adrien/Documents/CS5304-DSW/assignments/assignment4/predictions.txt\",\n",
      "    \"use_embeddings\": 2\n",
      "}\n",
      "Reading data dictionary_path=~\\Desktop\\DataScience\\assign4/dictionary.txt phrase_ids_path=/home/adrien/Documents/CS5304-DSW/assignments/assignment4/phrase_ids.train.txt labels_path=~\\Desktop\\DataScience\\assign4/sentiment_labels.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~\\\\Desktop\\\\DataScience\\\\assign4/dictionary.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b3e7cbe5c324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-8f46c91cc935>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(options)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m     \u001b[0mload_data_and_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mEPOCH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-f51b34699209>\u001b[0m in \u001b[0;36mload_data_and_embeddings\u001b[0;34m(data_path, phrase_ids_path, embeddings_path)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mlabels_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentiment_labels.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mdictionary_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dictionary.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_dictionary_txt_with_phrase_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase_ids_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'phrase_ids.train.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_dictionary_txt_with_phrase_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase_ids_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'phrase_ids.dev.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_dictionary_txt_with_phrase_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase_ids_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'phrase_ids.test.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-f51b34699209>\u001b[0m in \u001b[0;36mread_dictionary_txt_with_phrase_ids\u001b[0;34m(dictionary_path, phrase_ids_path, labels_path)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mphrase_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mexamples_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~\\\\Desktop\\\\DataScience\\\\assign4/dictionary.txt'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--ids', default=mydir, type=str)\n",
    "\n",
    "    parser.add_argument('--data', default=os.path.expanduser('data/stanfordSentimentTreebank'), type=str)\n",
    "\n",
    "    parser.add_argument('--embeddings', default=os.path.expanduser('data/GoogleNews-vectors-negative300.txt'), type=str)\n",
    "\n",
    "    parser.add_argument('--model', default=os.path.join(mydir, 'model.ckpt'), type=str)\n",
    "\n",
    "    parser.add_argument('--predictions', default=os.path.join(mydir, 'predictions.txt'), type=str)\n",
    "\n",
    "    parser.add_argument('--log_every', default=100, type=int)\n",
    "\n",
    "    parser.add_argument('--eval_every', default=1000, type=int)\n",
    "\n",
    "    parser.add_argument('--batch_size', default=32, type=int)\n",
    "\n",
    "    parser.add_argument('--eval_only_mode', action='store_true')\n",
    "\n",
    "    parser.add_argument('--embeddings2', default=os.path.expanduser('data/glove/glove.840B.300d.txt'), type=str)\n",
    "    \n",
    "    parser.add_argument('--use_embeddings', default=2, type=int)\n",
    "    \n",
    "    sys.argv = ['foo']\n",
    "    options = parser.parse_args()\n",
    "\n",
    "    print(json.dumps(options.__dict__, sort_keys=True, indent=4))\n",
    "\n",
    "\n",
    "\n",
    "    run(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
