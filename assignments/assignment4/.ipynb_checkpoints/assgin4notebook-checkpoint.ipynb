{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PAD_TOKEN = '_PAD_'\n",
    "UNK_TOKEN = '_UNK_'\n",
    "\n",
    "\n",
    "mydir = os.path.dirname(os.path.abspath('data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/adrien/Documents/CS5304-DSW/assignments/assignment4'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "class BagOfWordsModel(nn.Module):\n",
    "  def __init__(self, embeddings):\n",
    "    super(BagOfWordsModel, self).__init__()\n",
    "        \n",
    "    self.embed = nn.Embedding(embeddings.shape[0], embeddings.shape[1], sparse=True)\n",
    "    self.embed.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "    self.classify = nn.Linear(embeddings.shape[1], 5)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.classify(self.embed(x).sum(1))\n",
    "\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, kernel_dim=100, kernel_sizes=(3, 4, 5), dropout=0.5):\n",
    "        super(CNNClassifier,self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, kernel_dim, (K, embedding_dim)) for K in kernel_sizes])\n",
    "\n",
    "        # kernal_size = (K,D) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * kernel_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def init_weights(self, pretrained_word_vectors, is_static=False):\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(pretrained_word_vectors).float())\n",
    "        if is_static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, inputs, is_training=False):\n",
    "        inputs = self.embedding(inputs).unsqueeze(1) # (B,1,T,D)\n",
    "        inputs = [F.relu(conv(inputs)).squeeze(3) for conv in self.convs] #[(N,Co,W), ...]*len(Ks)\n",
    "        inputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in inputs] #[(N,Co), ...]*len(Ks)\n",
    "\n",
    "        concated = torch.cat(inputs, 1)\n",
    "\n",
    "        if is_training:\n",
    "            concated = self.dropout(concated) # (N,len(Ks)*Co)\n",
    "        out = self.fc(concated) \n",
    "        return F.log_softmax(out,1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenConverter(object):\n",
    "  def __init__(self, vocab):\n",
    "    self.vocab = vocab\n",
    "    self.unknown = 0\n",
    "\n",
    "  def convert(self, token):\n",
    "    if token in self.vocab:\n",
    "      id = self.vocab.get(token)\n",
    "    else:\n",
    "      id = self.vocab.get(UNK_TOKEN)\n",
    "      self.unknown += 1\n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods for loading SST data\n",
    "\n",
    "def sentiment2label(x):\n",
    "  if x >= 0 and x <= 0.2:\n",
    "    return 0\n",
    "  elif x > 0.2 and x <= 0.4:\n",
    "    return 1\n",
    "  elif x > 0.4 and x <= 0.6:\n",
    "    return 2\n",
    "  elif x > 0.6 and x <= 0.8:\n",
    "    return 3\n",
    "  elif x > 0.8 and x <= 1:\n",
    "    return 4\n",
    "  else:\n",
    "    raise ValueError('Improper sentiment value {}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeWord(word,lemmatizer,position='n'):\n",
    "    #this function lemmatizes the word given\n",
    "    \n",
    "    newWord = lemmatizer.lemmatize(word,pos=position)\n",
    "    \n",
    "    if position == 'n':\n",
    "        nextPosition = 'v'\n",
    "    elif position == 'v':\n",
    "        nextPosition = 'a'\n",
    "    elif position == 'a':\n",
    "        nextPosition = 'r'\n",
    "    else:\n",
    "        return newWord\n",
    "    \n",
    "    if newWord == word:\n",
    "        #nothing changed, try something else\n",
    "        newWord = lemmatizeWord(word,lemmatizer,nextPosition)\n",
    "    \n",
    "    return newWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toeknizePhrase(phrase):\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    \n",
    "    print(phrase)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    phraseWords = phrase.split(' ')\n",
    "    for word in phraseWords:\n",
    "        newWord = lemmatizeWord(word,lemmatizer)\n",
    "        print(newWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dictionary_txt_with_phrase_ids(dictionary_path, phrase_ids_path, labels_path=None):\n",
    "  print('Reading data dictionary_path={} phrase_ids_path={} labels_path={}'.format(\n",
    "    dictionary_path, phrase_ids_path, labels_path))\n",
    "\n",
    "  with open(phrase_ids_path) as f:\n",
    "    phrase_ids = set(line.strip() for line in f)\n",
    "\n",
    "  with open(dictionary_path) as f:\n",
    "    examples_dict = dict()\n",
    "    for line in f:\n",
    "      print(line)\n",
    "      parts = line.strip().split('|')\n",
    "      phrase = parts[0]\n",
    "      phrase_id = parts[1]\n",
    "\n",
    "      if phrase_id not in phrase_ids:\n",
    "        continue\n",
    "\n",
    "      example = dict()\n",
    "      example['phrase'] = phrase.replace('(', '-LRB').replace(')', '-RRB-')\n",
    "      example['tokens'] = toeknizePhrase(example['phrase'])\n",
    "      example['example_id'] = phrase_id\n",
    "      example['label'] = None\n",
    "    \n",
    "    \n",
    "      print(example)\n",
    "    \n",
    "      examples_dict[example['example_id']] = example\n",
    "\n",
    "  if labels_path is not None:\n",
    "    with open(labels_path) as f:\n",
    "      for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "          continue\n",
    "        parts = line.strip().split('|')\n",
    "        phrase_id = parts[0]\n",
    "        sentiment = float(parts[1])\n",
    "        label = sentiment2label(sentiment)\n",
    "\n",
    "        if phrase_id in examples_dict:\n",
    "          examples_dict[phrase_id]['label'] = label\n",
    "\n",
    "  examples = [ex for _, ex in examples_dict.items()]\n",
    "\n",
    "  print('Found {} examples.'.format(len(examples)))\n",
    "\n",
    "  return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(datasets):\n",
    "    vocab = dict()\n",
    "    vocab[PAD_TOKEN] = len(vocab)\n",
    "    vocab[UNK_TOKEN] = len(vocab)\n",
    "    for data in datasets:\n",
    "        for example in data:\n",
    "          for word in example['tokens']:\n",
    "            if word not in vocab:\n",
    "              vocab[word] = len(vocab)\n",
    "\n",
    "    print('Vocab size: {}'.format(len(vocab)))\n",
    "#     pint(vocab)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2ids(data, vocab):\n",
    "  converter = TokenConverter(vocab)\n",
    "  for example in data:\n",
    "    example['tokens'] = list(map(converter.convert, example['tokens']))\n",
    "  print('Found {} unknown tokens.'.format(converter.unknown))\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_embeddings(data_path, phrase_ids_path, embeddings_path):\n",
    "  labels_path = os.path.join(data_path, 'sentiment_labels.txt')\n",
    "  dictionary_path = os.path.join(data_path, 'dictionary.txt')\n",
    "  train_data = read_dictionary_txt_with_phrase_ids(dictionary_path, os.path.join(phrase_ids_path, 'phrase_ids.train.txt'), labels_path)\n",
    "  validation_data = read_dictionary_txt_with_phrase_ids(dictionary_path, os.path.join(phrase_ids_path, 'phrase_ids.dev.txt'), labels_path)\n",
    "  test_data = read_dictionary_txt_with_phrase_ids(dictionary_path, os.path.join(phrase_ids_path, 'phrase_ids.test.txt'))\n",
    "  vocab = build_vocab([train_data, validation_data, test_data])\n",
    "  vocab, embeddings = load_embeddings(options.embeddings, vocab, cache=True)\n",
    "  train_data = convert2ids(train_data, vocab)\n",
    "  validation_data = convert2ids(validation_data, vocab)\n",
    "  test_data = convert2ids(test_data, vocab)\n",
    "  return train_data, validation_data, test_data, vocab, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path, vocab, cache=False, cache_path=None):\n",
    "  print(\"Loading Embeddings\")\n",
    "#   print(vocab)\n",
    "  rows = []\n",
    "  new_vocab = [UNK_TOKEN]\n",
    "\n",
    "  if cache_path is None:\n",
    "    cache_path = path + '.cache'\n",
    "\n",
    "  # Use cache file if it exists.\n",
    "  if os.path.exists(cache_path):\n",
    "    path = cache_path\n",
    "\n",
    "  print(\"Reading embeddings from {}\".format(path))\n",
    "\n",
    "  # first pass over the embeddings to vocab and relevant rows\n",
    "  with open(path) as f:\n",
    "    for line in f:\n",
    "      word, row = line.split(' ', 1)\n",
    "      if word == UNK_TOKEN:\n",
    "        raise ValueError('The unk token should not exist w.in embeddings.')\n",
    "      if word in vocab:\n",
    "        rows.append(line)\n",
    "        new_vocab.append(word)\n",
    "\n",
    "  # optionally save relevant rows to cache file.\n",
    "  if cache and not os.path.exists(cache_path):\n",
    "    with open(cache_path, 'w') as f:\n",
    "      for line in rows:\n",
    "        f.write(line)\n",
    "      print(\"Cached embeddings to {}\".format(cache_path))\n",
    "\n",
    "  # turn vocab list into a dictionary\n",
    "  new_vocab = {w: i for i, w in enumerate(new_vocab)}\n",
    "\n",
    "  print('New vocab size: {}'.format(len(new_vocab)))\n",
    "\n",
    "  assert len(rows) == len(new_vocab) - 1\n",
    "\n",
    "  # create embeddings matrix\n",
    "  embeddings = np.zeros((len(new_vocab), 300), dtype=np.float32)\n",
    "  for i, line in enumerate(rows):\n",
    "    embeddings[i+1] = list(map(float, line.strip().split(' ')[1:]))\n",
    "    \n",
    "  print(new_vocab)\n",
    "  return new_vocab, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Iterator\n",
    "\n",
    "def prepare_data(data):\n",
    "    print(\"Preparing data\")\n",
    "    # pad data\n",
    "    maxlen = max(map(len, data))\n",
    "    data = [ex + [0] * (maxlen-len(ex)) for ex in data]\n",
    "#     print(data)\n",
    "\n",
    "    # wrap in tensor\n",
    "    return torch.LongTensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels(labels):\n",
    "  try:\n",
    "    return torch.LongTensor(labels)\n",
    "  except:\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(dataset, batch_size, forever=False):\n",
    "  dataset_size = len(dataset)\n",
    "  order = None\n",
    "  nbatches = dataset_size // batch_size\n",
    "\n",
    "  def init_order():\n",
    "    return random.sample(range(dataset_size), dataset_size)\n",
    "\n",
    "  def get_batch(start, end):\n",
    "    batch = [dataset[ii] for ii in order[start:end]]\n",
    "    data = prepare_data([ex['tokens'] for ex in batch])\n",
    "    labels = prepare_labels([ex['label'] for ex in batch])\n",
    "    example_ids = [ex['example_id'] for ex in batch]\n",
    "    return data, labels, example_ids\n",
    "\n",
    "  order = init_order()\n",
    "\n",
    "  while True:\n",
    "    for i in range(nbatches):\n",
    "      start = i*batch_size\n",
    "      end = (i+1)*batch_size\n",
    "      yield get_batch(start, end)\n",
    "\n",
    "    if nbatches*batch_size < dataset_size:\n",
    "      yield get_batch(nbatches*batch_size, dataset_size)\n",
    "\n",
    "    if not forever:\n",
    "      break\n",
    "    \n",
    "    order = init_order()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Methods\n",
    "\n",
    "def checkpoint_model(step, val_err, model, opt, save_path):\n",
    "  save_dict = dict(\n",
    "    step=step,\n",
    "    val_err=val_err,\n",
    "    model_state_dict=model.state_dict(),\n",
    "    opt_state_dict=opt.state_dict())\n",
    "  torch.save(save_dict, save_path)\n",
    "\n",
    "\n",
    "def load_model(model, opt, load_path):\n",
    "  load_dict = torch.load(load_path)\n",
    "  step = load_dict['step']\n",
    "  val_err = load_dict['val_err']\n",
    "  model.load_state_dict(load_dict['model_state_dict'])\n",
    "  opt.load_state_dict(load_dict['opt_state_dict'])\n",
    "  return step, val_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Namespace object at 0x7fc97743ca90>\n",
      "Reading data dictionary_path=/home/adrien/Documents/CS5304-DSW/assignments/assignment4/data/stanfordSentimentTreebank/dictionary.txt phrase_ids_path=/home/adrien/Documents/CS5304-DSW/assignments/assignment4/phrase_ids.train.txt labels_path=/home/adrien/Documents/CS5304-DSW/assignments/assignment4/data/stanfordSentimentTreebank/sentiment_labels.txt\n",
      "!|0\n",
      "\n",
      "[nltk_data] Downloading package stopwords to /home/adrien/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/adrien/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "lemmatize() takes from 2 to 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-792fde823086>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-93-792fde823086>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(options)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m     \u001b[0mload_data_and_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBagOfWordsModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# change this if we create a different class names than bag of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-2b4950931383>\u001b[0m in \u001b[0;36mload_data_and_embeddings\u001b[0;34m(data_path, phrase_ids_path, embeddings_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mlabels_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentiment_labels.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mdictionary_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dictionary.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_dictionary_txt_with_phrase_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase_ids_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'phrase_ids.train.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_dictionary_txt_with_phrase_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase_ids_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'phrase_ids.dev.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_dictionary_txt_with_phrase_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase_ids_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'phrase_ids.test.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-92-15edbc235a90>\u001b[0m in \u001b[0;36mread_dictionary_txt_with_phrase_ids\u001b[0;34m(dictionary_path, phrase_ids_path, labels_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phrase'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'('\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-LRB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m')'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-RRB-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoeknizePhrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phrase'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'example_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphrase_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-02a9c2749135>\u001b[0m in \u001b[0;36mtoeknizePhrase\u001b[0;34m(phrase)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mphraseWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mphraseWords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mnewWord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatizeWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewWord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-04fc02878948>\u001b[0m in \u001b[0;36mlemmatizeWord\u001b[0;34m(word, lemmatizer, position)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnewWord\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#nothing changed, try something else\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mnewWord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnextPosition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnewWord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: lemmatize() takes from 2 to 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "\n",
    "def run_validation(model, dataset, options):\n",
    "  err = 0\n",
    "  count = 0\n",
    "  for data, labels, _ in batch_iterator(dataset, options.batch_size, forever=False):\n",
    "    outp = model(Variable(data))\n",
    "    loss = nn.NLLLoss()(F.log_softmax(outp), Variable(labels))\n",
    "    acc = (outp.data.max(1)[1] == labels).sum() / data.shape[0]\n",
    "    err += (1-acc) * data.shape[0]\n",
    "    count += data.shape[0]\n",
    "  err = err / count\n",
    "  print('Ev-Err={}'.format(err))\n",
    "  return err\n",
    "\n",
    "\n",
    "def run_test(model, dataset, options):\n",
    "  print('Writing predictions to {}'.format(os.path.abspath(options.predictions)))\n",
    "\n",
    "  preds_dict = dict()\n",
    "\n",
    "  for data, _, example_ids in batch_iterator(dataset, options.batch_size, forever=False):\n",
    "    outp = model(Variable(data))\n",
    "    preds = outp.data.max(1)[1]\n",
    "\n",
    "    for id, pred in zip(example_ids, preds):\n",
    "      preds_dict[id] = pred\n",
    "\n",
    "  with open(options.predictions, 'w') as f:\n",
    "    for id, pred in preds_dict.items():\n",
    "      f.write('{}|{}\\n'.format(id, pred))\n",
    "\n",
    "\n",
    "def run(options):\n",
    "  train_data, validation_data, test_data, vocab, embeddings = \\\n",
    "    load_data_and_embeddings(options.data, options.ids, options.embeddings)\n",
    "  model = BagOfWordsModel(embeddings) # change this if we create a different class names than bag of words\n",
    "  opt = optim.SGD(model.parameters(), lr=3e-4)\n",
    "  \n",
    "  step = 0\n",
    "  best_val_err = 1\n",
    "\n",
    "  if options.eval_only_mode:\n",
    "    step, best_val_err = load_model(model, opt, options.model)\n",
    "    print('Model loaded from {}\\nstep={} best_val_err={}'.format(options.model, step, best_val_err))\n",
    "    run_test(model, test_data, options)\n",
    "    sys.exit()\n",
    "  \n",
    "  for data, labels, _ in batch_iterator(train_data, options.batch_size, forever=True):\n",
    "#     print(data)\n",
    "    outp = model(Variable(data))\n",
    "    loss = nn.NLLLoss()(F.log_softmax(outp), Variable(labels))\n",
    "    acc = (outp.data.max(1)[1] == labels).sum() / data.shape[0]\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    if step % options.log_every == 0:\n",
    "      print('Step={} Tr-Loss={} Tr-Acc={}'.format(step, loss.data[0], acc))\n",
    "      \n",
    "    if step % options.eval_every == 0:\n",
    "      val_err = run_validation(model, validation_data, options)\n",
    "      \n",
    "      # early stopping\n",
    "      if val_err < best_val_err:\n",
    "        best_val_err = val_err\n",
    "        print('Checkpointing model step={} best_val_err={}.'.format(step, best_val_err))\n",
    "        checkpoint_model(step, val_err, model, opt, options.model)\n",
    "    \n",
    "    step += 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#   parser = argparse.ArgumentParser()\n",
    "#   parser.add_argument('--ids', default=mydir, type=str)\n",
    "#   parser.add_argument('--data', default=os.path.expanduser('~/data/stanfordSentimentTreebank'), type=str)\n",
    "#   parser.add_argument('--embeddings', default=, type=str)\n",
    "#   parser.add_argument('--model', default=os.path.join(mydir, 'model.ckpt'), type=str)\n",
    "#   parser.add_argument('--predictions', default=os.path.join(mydir, 'predictions.txt'), type=str)\n",
    "#   parser.add_argument('--log_every', default=100, type=int)\n",
    "#   parser.add_argument('--eval_every', default=1000, type=int)\n",
    "#   parser.add_argument('--batch_size', default=32, type=int)\n",
    "#   parser.add_argument('--eval_only_mode', action='store_true')\n",
    "#   options = parser.parse_args()\n",
    "\n",
    "#   print(json.dumps(options.__dict__, sort_keys=True, indent=4))\n",
    "    \n",
    "    options = Namespace(\n",
    "        ids=mydir,\n",
    "        data=os.path.expanduser(mydir + '/data/stanfordSentimentTreebank'),\n",
    "        embeddings = os.path.expanduser(mydir +'/data/glove/glove.840B.300d.txt'),\n",
    "        model = os.path.join(mydir, 'model.ckpt'),\n",
    "        predictions = os.path.join(mydir, 'predictions.txt'),\n",
    "        log_every = 100,\n",
    "        batch_size = 32,\n",
    "        eval_only_mode = False,\n",
    "        eval_every=1000\n",
    "    )\n",
    "    \n",
    "    \n",
    "#     {\n",
    "#         ids:mydir,\n",
    "#         data: os.path.expanduser(mydir + '/data/stanfordSentimentTreebank'),\n",
    "#         'embeddings':0\n",
    "#     }\n",
    "    print(options)\n",
    "    \n",
    "    run(options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
