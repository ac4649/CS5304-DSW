{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import itertools\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Additional helper functions required for the code to run:\n",
    "# This function defines the column headers for the data frames\n",
    "def getColumnHeaders():\n",
    "    return pd.Series(data=['Index','label','integer_1','integer_2','integer_3',\n",
    "                                 'integer_4','integer_5','integer_6','integer_7','integer_8','integer_9',\n",
    "                                 'integer_10','integer_11','integer_12','integer_13','categorical_1',\n",
    "                                 'categorical_2','categorical_3','categorical_4','categorical_5','categorical_6',\n",
    "                                 'categorical_7','categorical_8','categorical_9','categorical_10','categorical_11',\n",
    "                                 'categorical_12','categorical_13','categorical_14','categorical_15','categorical_16',\n",
    "                                 'categorical_17','categorical_18','categorical_19','categorical_20','categorical_21',\n",
    "                                 'categorical_22','categorical_23','categorical_24','categorical_25','categorical_26'])\n",
    "\n",
    "# this function generateNIndecesFrom takes a range of Indeces and randomly takes n of them, returns a pandas Series object\n",
    "def generateNIndecesFrom(n, rangeOfIndeces):\n",
    "    # print(\"Generating \" + str(n) + \" indeces from range\")\n",
    "    allIndeces = random.sample(rangeOfIndeces, n)\n",
    "    allIndeces = pd.Series(data = allIndeces)\n",
    "    allIndeces = allIndeces.sort_values().reset_index().drop(['index'],axis=1)\n",
    "    allIndeces.columns = ['Index'];\n",
    "    return allIndeces\n",
    "\n",
    "# the generateSubSet function takes in a file, a dataFrame to put the data in as well as index values which should be extracted,\n",
    "# a number of rows per itteration (this is used to not overload the memory) , total number of rows in the file, the column headers for the new dataframe\n",
    "def generateSubSet(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers):\n",
    "    totalNumIterations = int(totalNumRows/numRowsPerItteration)\n",
    "    # print(\"Number of itterations = \" + str(totalNumIterations))\n",
    "    totalNumRowsTraversed = 0\n",
    "    prevsize = 0\n",
    "    for i in range(totalNumIterations + 1):\n",
    "#         \n",
    "#         print(\"Itteration number: \" + str(i))\n",
    "#         print(\"skipRows: \" + str(i * numRowsPerItteration))\n",
    "#         print(\"Read in : \" + str(numRowsPerItteration))\n",
    "        curData = pd.read_table(file,skiprows = i * numRowsPerItteration, nrows = numRowsPerItteration,header=None)\n",
    "        curData.index = [i for i in range(i*numRowsPerItteration,i*numRowsPerItteration + curData.shape[0])]\n",
    "        totalNumRowsTraversed = totalNumRowsTraversed + curData.shape[0]\n",
    "        clear_output()\n",
    "#         print(curData.head())\n",
    "#         print(curData.shape)\n",
    "#         print(curData.index.shape)\n",
    "\n",
    "        curData['Index'] = curData.index\n",
    "        curData.columns = column_headers\n",
    "        \n",
    "        curIndexRange = indexValues['Index'][(indexValues['Index'] < (i*numRowsPerItteration + numRowsPerItteration)) & (indexValues['Index'] > (i*numRowsPerItteration-1))]\n",
    "        print(curIndexRange)\n",
    "        print(curData.head())\n",
    "        print(curData['Index'])\n",
    "        print(curData['Index'].isin(curIndexRange))\n",
    "        curData = curData[curData['Index'].isin(list(curIndexRange))] # this line isn't working for some reason\n",
    "#         print(curData.head())\n",
    "        \n",
    "        dataFrame = pd.concat([dataFrame,curData])\n",
    "        \n",
    "                \n",
    "        \n",
    "#         print(dataFrame.head())\n",
    "#         print(curData.head())\n",
    "        \n",
    "        \n",
    "        print(\"Extraction Stats: \" + str(dataFrame.shape[0]) + \" percent: \" + str(dataFrame.shape[0] / indexValues.shape[0] * 100) + \"%\")\n",
    "        print(\"Document Stats: \" + str(totalNumRowsTraversed) + \" percent: \" + str(totalNumRowsTraversed/totalNumRows*100) + \"%\")\n",
    "        if (dataFrame.shape[0] - prevsize) > 500000:\n",
    "            prevsize = dataFrame.shape[0]\n",
    "#             dataFrame.to_csv(frameSaveName)\n",
    "#         elif dataFrame.shape[0] == indexValues.shape[0]:\n",
    "#             print(\"Finished with the data collection\")\n",
    "# #             dataFrame.to_csv(frameSaveName)\n",
    "#             break\n",
    "    # print(\"Extraction is Done, now saving frame\")        \n",
    "    return dataFrame\n",
    "\n",
    "# This method generates is a wrapper around the generateSubset to generate the subset and save the dataframe to a csv file (for being able to make use of it after)\n",
    "def generateAndSaveSubset(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers,frameSaveName):\n",
    "    dataFrame = generateSubSet(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers)\n",
    "    dataFrame.to_csv(frameSaveName)\n",
    "    return dataFrame\n",
    "\n",
    "def read_data(data_path, train_path, validation_path, test_path):\n",
    "\n",
    "    print(data_path)\n",
    "    print(train_path)\n",
    "    print(validation_path)\n",
    "    print(test_path)\n",
    "    \n",
    "    #get the ids\n",
    "    try:\n",
    "        trainIndeces = pd.read_csv(train_path, header = None)\n",
    "        validationIndeces = pd.read_csv(validation_path, header = None)\n",
    "        testingIndeces = pd.read_csv(test_path, header = None)\n",
    "    except:\n",
    "        print(\"There were not 1000000 data points\")\n",
    "        trainIndeces = generateNIndecesFrom(1000000,list(twoMIndeces['Index']))\n",
    "        trainIndeces.to_csv('train_ids.txt',index=False,header=False)\n",
    "\n",
    "        remainingIndeces = twoMIndeces['Index'][~twoMIndeces['Index'].isin(trainIndeces.values)]\n",
    "        validationIndeces = generateNIndecesFrom(250000,list(remainingIndeces))\n",
    "        validationIndeces.to_csv('validation_ids.txt',index=False,header=False)\n",
    "\n",
    "        testingIndeces = twoMIndeces['Index'][~(twoMIndeces['Index'].isin(trainIndeces.values) | twoMIndeces['Index'].isin(validationIndeces.values))]\n",
    "        testingIndeces = generateNIndecesFrom(750000,list(testingIndeces))\n",
    "        testingIndeces.to_csv('test_ids.txt',index=False,header=False)\n",
    "    \n",
    "    trainIndeces.columns = ['Index']\n",
    "    validationIndeces.columns = ['Index']\n",
    "    testingIndeces.columns = ['Index']\n",
    "#     print(trainIndeces.head())\n",
    "#     print(validationIndeces.head())\n",
    "#     print(testingIndeces.head())\n",
    "    \n",
    "    # Generate the actual data files\n",
    "    column_headers = getColumnHeaders()\n",
    "    # print(\"No 1M collection\")\n",
    "    train1M = pd.DataFrame()\n",
    "    train1M = generateSubSet(data_path,train1M,trainIndeces,4000000,46000000,column_headers)\n",
    "#     print(train1M.head())\n",
    "\n",
    "    # print(\"No 250k collection\")\n",
    "    validation250k = pd.DataFrame()\n",
    "    validation250k = generateSubSet(data_path,validation250k,validationIndeces,4000000,46000000,column_headers)\n",
    "\n",
    "\n",
    "    # print(\"No 750k collection\")\n",
    "    test750k = pd.DataFrame()\n",
    "    test750k = generateSubSet(data_path,test750k,validationIndeces,4000000,46000000,column_headers)\n",
    "    \n",
    "    \n",
    "    print(train1M.shape)\n",
    "    print(validation250k.shape)\n",
    "    print(test750k.shape)\n",
    "\n",
    "    # train_data, train_target = np.zeros((1000000, 39)), np.zeros((1000000,))\n",
    "    # validation_data, validation_target = np.zeros((250000, 39)), np.zeros((250000,))\n",
    "    # test_data, test_target = np.zeros((750000, 39)), np.zeros((750000,))\n",
    "\n",
    "\n",
    "    return train1M[train1M.columns[1:40]], train1M['label'].values(), validation250k[validation250k.columns[1:40]], validation250k['label'].values(), test750k[test750k.columns[1:40]], test750k['label'].values()\n",
    "#     return \n",
    "\n",
    "def preprocess_int_data(data, features):\n",
    "    n = len([f for f in features if f < 13])\n",
    "    return np.zeros((data.shape[0], n))\n",
    "\n",
    "\n",
    "def preprocess_cat_data(data, features, preprocess):\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174728     8000008\n",
      "174729     8000050\n",
      "174730     8000097\n",
      "174731     8000106\n",
      "174732     8000144\n",
      "174733     8000147\n",
      "174734     8000151\n",
      "174735     8000246\n",
      "174736     8000480\n",
      "174737     8000516\n",
      "174738     8000662\n",
      "174739     8000673\n",
      "174740     8000705\n",
      "174741     8000721\n",
      "174742     8000845\n",
      "174743     8000927\n",
      "174744     8001017\n",
      "174745     8001057\n",
      "174746     8001098\n",
      "174747     8001105\n",
      "174748     8001151\n",
      "174749     8001167\n",
      "174750     8001210\n",
      "174751     8001218\n",
      "174752     8001276\n",
      "174753     8001282\n",
      "174754     8001404\n",
      "174755     8001437\n",
      "174756     8001498\n",
      "174757     8001516\n",
      "            ...   \n",
      "262203    11998862\n",
      "262204    11998872\n",
      "262205    11998914\n",
      "262206    11998927\n",
      "262207    11998930\n",
      "262208    11999003\n",
      "262209    11999026\n",
      "262210    11999098\n",
      "262211    11999292\n",
      "262212    11999326\n",
      "262213    11999359\n",
      "262214    11999369\n",
      "262215    11999389\n",
      "262216    11999406\n",
      "262217    11999444\n",
      "262218    11999456\n",
      "262219    11999488\n",
      "262220    11999493\n",
      "262221    11999498\n",
      "262222    11999504\n",
      "262223    11999588\n",
      "262224    11999756\n",
      "262225    11999764\n",
      "262226    11999788\n",
      "262227    11999798\n",
      "262228    11999817\n",
      "262229    11999832\n",
      "262230    11999909\n",
      "262231    11999953\n",
      "262232    11999979\n",
      "Name: Index, Length: 87505, dtype: int64\n",
      "         Index  label  integer_1  integer_2  integer_3  integer_4  integer_5  \\\n",
      "8000000      0    NaN         52       29.0       10.0    18434.0       48.0   \n",
      "8000001      0    NaN         -1        4.0        1.0    17236.0       19.0   \n",
      "8000002      0    1.0          0       14.0       14.0     1149.0       21.0   \n",
      "8000003      0    1.0        441        1.0        6.0     1329.0       28.0   \n",
      "8000004      0    NaN        123        NaN        NaN     4445.0        8.0   \n",
      "\n",
      "         integer_6  integer_7  integer_8      ...        categorical_17  \\\n",
      "8000000        5.0       10.0       24.0      ...              891589e7   \n",
      "8000001        1.0        1.0       19.0      ...              b04e4670   \n",
      "8000002        8.0       28.0      301.0      ...              005c6740   \n",
      "8000003        7.0       28.0      145.0      ...              ac7705cd   \n",
      "8000004        2.0        0.0        0.0      ...              0c4e94df   \n",
      "\n",
      "         categorical_18  categorical_19  categorical_20 categorical_21  \\\n",
      "8000000        0a948b5c        b1252a9d        ebb9df6b            NaN   \n",
      "8000001        21ddcdc9        5840adea        60f6221e            NaN   \n",
      "8000002        21ddcdc9        5840adea        8717ea07            NaN   \n",
      "8000003             NaN             NaN        7edbd38b            NaN   \n",
      "8000004        21ddcdc9        5840adea        226995b8            NaN   \n",
      "\n",
      "        categorical_22 categorical_23 categorical_24 categorical_25  \\\n",
      "8000000       32c7478e       3fdb382b       ea9a246c       49d68486   \n",
      "8000001       32c7478e       43f13e8b       ea9a246c       731c3655   \n",
      "8000002       3a171ecb       1793a828       e8b83407       b9809574   \n",
      "8000003       32c7478e       4fcc135f            NaN            NaN   \n",
      "8000004       32c7478e       2543e480       ea9a246c       fcd456fa   \n",
      "\n",
      "        categorical_26  \n",
      "8000000        8000000  \n",
      "8000001        8000001  \n",
      "8000002        8000002  \n",
      "8000003        8000003  \n",
      "8000004        8000004  \n",
      "\n",
      "[5 rows x 41 columns]\n",
      "8000000     0\n",
      "8000001     0\n",
      "8000002     0\n",
      "8000003     0\n",
      "8000004     0\n",
      "8000005     0\n",
      "8000006     0\n",
      "8000007     0\n",
      "8000008     0\n",
      "8000009     0\n",
      "8000010     0\n",
      "8000011     0\n",
      "8000012     0\n",
      "8000013     0\n",
      "8000014     1\n",
      "8000015     0\n",
      "8000016     0\n",
      "8000017     1\n",
      "8000018     1\n",
      "8000019     1\n",
      "8000020     1\n",
      "8000021     0\n",
      "8000022     1\n",
      "8000023     0\n",
      "8000024     0\n",
      "8000025     0\n",
      "8000026     1\n",
      "8000027     0\n",
      "8000028     0\n",
      "8000029     0\n",
      "           ..\n",
      "11999970    0\n",
      "11999971    1\n",
      "11999972    0\n",
      "11999973    0\n",
      "11999974    0\n",
      "11999975    0\n",
      "11999976    0\n",
      "11999977    0\n",
      "11999978    1\n",
      "11999979    0\n",
      "11999980    0\n",
      "11999981    1\n",
      "11999982    1\n",
      "11999983    0\n",
      "11999984    0\n",
      "11999985    0\n",
      "11999986    0\n",
      "11999987    1\n",
      "11999988    1\n",
      "11999989    1\n",
      "11999990    0\n",
      "11999991    0\n",
      "11999992    0\n",
      "11999993    0\n",
      "11999994    0\n",
      "11999995    0\n",
      "11999996    0\n",
      "11999997    0\n",
      "11999998    0\n",
      "11999999    1\n",
      "Name: Index, Length: 4000000, dtype: int64\n",
      "8000000     False\n",
      "8000001     False\n",
      "8000002     False\n",
      "8000003     False\n",
      "8000004     False\n",
      "8000005     False\n",
      "8000006     False\n",
      "8000007     False\n",
      "8000008     False\n",
      "8000009     False\n",
      "8000010     False\n",
      "8000011     False\n",
      "8000012     False\n",
      "8000013     False\n",
      "8000014     False\n",
      "8000015     False\n",
      "8000016     False\n",
      "8000017     False\n",
      "8000018     False\n",
      "8000019     False\n",
      "8000020     False\n",
      "8000021     False\n",
      "8000022     False\n",
      "8000023     False\n",
      "8000024     False\n",
      "8000025     False\n",
      "8000026     False\n",
      "8000027     False\n",
      "8000028     False\n",
      "8000029     False\n",
      "            ...  \n",
      "11999970    False\n",
      "11999971    False\n",
      "11999972    False\n",
      "11999973    False\n",
      "11999974    False\n",
      "11999975    False\n",
      "11999976    False\n",
      "11999977    False\n",
      "11999978    False\n",
      "11999979    False\n",
      "11999980    False\n",
      "11999981    False\n",
      "11999982    False\n",
      "11999983    False\n",
      "11999984    False\n",
      "11999985    False\n",
      "11999986    False\n",
      "11999987    False\n",
      "11999988    False\n",
      "11999989    False\n",
      "11999990    False\n",
      "11999991    False\n",
      "11999992    False\n",
      "11999993    False\n",
      "11999994    False\n",
      "11999995    False\n",
      "11999996    False\n",
      "11999997    False\n",
      "11999998    False\n",
      "11999999    False\n",
      "Name: Index, Length: 4000000, dtype: bool\n",
      "Extraction Stats: 0 percent: 0.0%\n",
      "Document Stats: 12000000 percent: 26.08695652173913%\n"
     ]
    }
   ],
   "source": [
    "def check_output_features(fs):\n",
    "    assert len(fs) < 39\n",
    "\n",
    "\n",
    "def check_output_read_data(data, target, n):\n",
    "    assert data.shape[0] == n\n",
    "    assert target.shape[0] == n\n",
    "\n",
    "\n",
    "def check_output_preprocess(preprocess):\n",
    "    assert (preprocess == 'onehot') or (preprocess == 'rate') or (preprocess == 'tfidf')\n",
    "\n",
    "\n",
    "def check_output_preprocess_int_data(data, fs):\n",
    "    n = len([f for f in fs if f < 13])\n",
    "    assert data.shape[1] == n\n",
    "\n",
    "\n",
    "def check_output_preprocess_cat_data(data, fs, preprocess):\n",
    "    pass\n",
    "\n",
    "\n",
    "def read_features(path):\n",
    "    features = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            features.append(int(line.strip()))\n",
    "    return features\n",
    "\n",
    "\n",
    "def read_preprocess(path):\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            return line.strip()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = './dac/train.txt'\n",
    "    train = 'train_ids.txt'\n",
    "    validation = 'validation_ids.txt'\n",
    "    test = 'test_ids.txt'\n",
    "    features = 'features.txt'\n",
    "    preprocess = 'preprocess.txt'\n",
    "\n",
    "    train_data, train_target, validation_data, validation_target, test_data, test_target = \\\n",
    "        read_data(data, train, validation, test)\n",
    "\n",
    "    check_output_read_data(train_data, train_target, 1000000)\n",
    "    check_output_read_data(validation_data, validation_target, 250000)\n",
    "    check_output_read_data(test_data, test_target, 750000)\n",
    "\n",
    "    features = read_features(options.features)\n",
    "\n",
    "    check_output_features(features)\n",
    "\n",
    "    preprocess = read_preprocess(options.preprocess)\n",
    "\n",
    "    check_output_preprocess(prepr3ocess)\n",
    "\n",
    "    train_int_data = preprocess_int_data(train_data, features)\n",
    "    validation_int_data = preprocess_int_data(validation_data, features)\n",
    "    test_int_data = preprocess_int_data(test_data, features)\n",
    "\n",
    "    check_output_preprocess_int_data(train_int_data, features)\n",
    "    check_output_preprocess_int_data(validation_int_data, features)\n",
    "    check_output_preprocess_int_data(test_int_data, features)\n",
    "\n",
    "    train_cat_data = preprocess_cat_data(train_data, features, preprocess)\n",
    "    validation_cat_data = preprocess_cat_data(validation_data, features, preprocess)\n",
    "    test_cat_data = preprocess_cat_data(test_data, features, preprocess)\n",
    "\n",
    "    check_output_preprocess_cat_data(train_cat_data, features, preprocess)\n",
    "    check_output_preprocess_cat_data(validation_cat_data, features, preprocess)\n",
    "    check_output_preprocess_cat_data(test_cat_data, features, preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
