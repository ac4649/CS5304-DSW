{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import itertools\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional helper functions required for the code to run:\n",
    "# This function defines the column headers for the data frames\n",
    "def getColumnHeaders():\n",
    "    return pd.Series(data=['Index','label','integer_1','integer_2','integer_3',\n",
    "                                 'integer_4','integer_5','integer_6','integer_7','integer_8','integer_9',\n",
    "                                 'integer_10','integer_11','integer_12','integer_13','categorical_1',\n",
    "                                 'categorical_2','categorical_3','categorical_4','categorical_5','categorical_6',\n",
    "                                 'categorical_7','categorical_8','categorical_9','categorical_10','categorical_11',\n",
    "                                 'categorical_12','categorical_13','categorical_14','categorical_15','categorical_16',\n",
    "                                 'categorical_17','categorical_18','categorical_19','categorical_20','categorical_21',\n",
    "                                 'categorical_22','categorical_23','categorical_24','categorical_25','categorical_26'])\n",
    "\n",
    "# this function generateNIndecesFrom takes a range of Indeces and randomly takes n of them, returns a pandas Series object\n",
    "def generateNIndecesFrom(n, rangeOfIndeces):\n",
    "    # print(\"Generating \" + str(n) + \" indeces from range\")\n",
    "    allIndeces = random.sample(rangeOfIndeces, n)\n",
    "    allIndeces = pd.Series(data = allIndeces)\n",
    "    allIndeces = allIndeces.sort_values().reset_index().drop(['index'],axis=1)\n",
    "    allIndeces.columns = ['Index'];\n",
    "    return allIndeces\n",
    "\n",
    "# the generateSubSet function takes in a file, a dataFrame to put the data in as well as index values which should be extracted,\n",
    "# a number of rows per itteration (this is used to not overload the memory) , total number of rows in the file, the column headers for the new dataframe\n",
    "def generateSubSet(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers):\n",
    "    totalNumIterations = int(totalNumRows/numRowsPerItteration)\n",
    "    # print(\"Number of itterations = \" + str(totalNumIterations))\n",
    "    totalNumRowsTraversed = 0\n",
    "    prevsize = 0\n",
    "    for i in range(totalNumIterations + 1):\n",
    "#         \n",
    "#         print(\"Itteration number: \" + str(i))\n",
    "#         print(\"skipRows: \" + str(i * numRowsPerItteration))\n",
    "#         print(\"Read in : \" + str(numRowsPerItteration))\n",
    "        curData = pd.read_table(file,skiprows = i * numRowsPerItteration, nrows = numRowsPerItteration,header=None)\n",
    "        curData.index = [i for i in range(i*numRowsPerItteration,i*numRowsPerItteration + curData.shape[0])]\n",
    "        totalNumRowsTraversed = totalNumRowsTraversed + curData.shape[0]\n",
    "        clear_output()\n",
    "#         print(curData.head())\n",
    "#         print(curData.shape)\n",
    "#         print(curData.index.shape)\n",
    "\n",
    "        curData['Index'] = curData.index\n",
    "        curData.columns = column_headers\n",
    "        \n",
    "        curIndexRange = indexValues['Index'][(indexValues['Index'] < (i*numRowsPerItteration + numRowsPerItteration)) & (indexValues['Index'] > (i*numRowsPerItteration-1))]\n",
    "        print(curIndexRange)\n",
    "        print(curData.head())\n",
    "        curData = curData[curData['Index'].isin(list(curIndexRange))] # this line isn't working for some reason\n",
    "        print(curData.head())\n",
    "        \n",
    "        dataFrame = pd.concat([dataFrame,curData])\n",
    "        \n",
    "                \n",
    "        \n",
    "        print(dataFrame.head())\n",
    "        print(curData.head())\n",
    "        \n",
    "        \n",
    "        print(\"Extraction Stats: \" + str(dataFrame.shape[0]) + \" percent: \" + str(dataFrame.shape[0] / indexValues.shape[0] * 100) + \"%\")\n",
    "        print(\"Document Stats: \" + str(totalNumRowsTraversed) + \" percent: \" + str(totalNumRowsTraversed/totalNumRows*100) + \"%\")\n",
    "        if (dataFrame.shape[0] - prevsize) > 500000:\n",
    "            prevsize = dataFrame.shape[0]\n",
    "#             dataFrame.to_csv(frameSaveName)\n",
    "#         elif dataFrame.shape[0] == indexValues.shape[0]:\n",
    "#             print(\"Finished with the data collection\")\n",
    "# #             dataFrame.to_csv(frameSaveName)\n",
    "#             break\n",
    "    # print(\"Extraction is Done, now saving frame\")        \n",
    "    return dataFrame\n",
    "\n",
    "# This method generates is a wrapper around the generateSubset to generate the subset and save the dataframe to a csv file (for being able to make use of it after)\n",
    "def generateAndSaveSubset(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers,frameSaveName):\n",
    "    dataFrame = generateSubSet(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers)\n",
    "    dataFrame.to_csv(frameSaveName)\n",
    "    return dataFrame\n",
    "\n",
    "def read_data(data_path, train_path, validation_path, test_path):\n",
    "\n",
    "    print(data_path)\n",
    "    print(train_path)\n",
    "    print(validation_path)\n",
    "    print(test_path)\n",
    "    \n",
    "    #get the ids\n",
    "    try:\n",
    "        trainIndeces = pd.read_csv(train_path, header = None)\n",
    "        validationIndeces = pd.read_csv(validation_path, header = None)\n",
    "        testingIndeces = pd.read_csv(test_path, header = None)\n",
    "    except:\n",
    "        print(\"There were not 1000000 data points\")\n",
    "        trainIndeces = generateNIndecesFrom(1000000,list(twoMIndeces['Index']))\n",
    "        trainIndeces.to_csv('train_ids.txt',index=False,header=False)\n",
    "\n",
    "        remainingIndeces = twoMIndeces['Index'][~twoMIndeces['Index'].isin(trainIndeces.values)]\n",
    "        validationIndeces = generateNIndecesFrom(250000,list(remainingIndeces))\n",
    "        validationIndeces.to_csv('validation_ids.txt',index=False,header=False)\n",
    "\n",
    "        testingIndeces = twoMIndeces['Index'][~(twoMIndeces['Index'].isin(trainIndeces.values) | twoMIndeces['Index'].isin(validationIndeces.values))]\n",
    "        testingIndeces = generateNIndecesFrom(750000,list(testingIndeces))\n",
    "        testingIndeces.to_csv('test_ids.txt',index=False,header=False)\n",
    "    \n",
    "    trainIndeces.columns = ['Index']\n",
    "    validationIndeces.columns = ['Index']\n",
    "    testingIndeces.columns = ['Index']\n",
    "#     print(trainIndeces.head())\n",
    "#     print(validationIndeces.head())\n",
    "#     print(testingIndeces.head())\n",
    "    \n",
    "    # Generate the actual data files\n",
    "    column_headers = getColumnHeaders()\n",
    "    # print(\"No 1M collection\")\n",
    "    train1M = pd.DataFrame()\n",
    "    train1M = generateSubSet(data_path,train1M,trainIndeces,4000000,46000000,column_headers)\n",
    "    print(train1M.head())\n",
    "\n",
    "    # print(\"No 250k collection\")\n",
    "    validation250k = pd.DataFrame()\n",
    "    validation250k = generateSubSet(data_path,validation250k,validationIndeces,4000000,46000000,column_headers)\n",
    "\n",
    "\n",
    "    # print(\"No 750k collection\")\n",
    "    test750k = pd.DataFrame()\n",
    "    test750k = generateSubSet(data_path,test750k,validationIndeces,4000000,46000000,column_headers)\n",
    "    \n",
    "    \n",
    "    print(train1M.shape)\n",
    "    print(validation250k.shape)\n",
    "    print(test750k.shape)\n",
    "\n",
    "    # train_data, train_target = np.zeros((1000000, 39)), np.zeros((1000000,))\n",
    "    # validation_data, validation_target = np.zeros((250000, 39)), np.zeros((250000,))\n",
    "    # test_data, test_target = np.zeros((750000, 39)), np.zeros((750000,))\n",
    "\n",
    "\n",
    "    return train1M[train1M.columns[1:40]], train1M['label'].values(), validation250k[validation250k.columns[1:40]], validation250k['label'].values(), test750k[test750k.columns[1:40]], test750k['label'].values()\n",
    "#     return \n",
    "\n",
    "def preprocess_int_data(data, features):\n",
    "    n = len([f for f in features if f < 13])\n",
    "    return np.zeros((data.shape[0], n))\n",
    "\n",
    "\n",
    "def preprocess_cat_data(data, features, preprocess):\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87443     4000075\n",
      "87444     4000109\n",
      "87445     4000172\n",
      "87446     4000199\n",
      "87447     4000251\n",
      "87448     4000262\n",
      "87449     4000366\n",
      "87450     4000372\n",
      "87451     4000388\n",
      "87452     4000389\n",
      "87453     4000466\n",
      "87454     4000554\n",
      "87455     4000571\n",
      "87456     4000619\n",
      "87457     4000628\n",
      "87458     4000651\n",
      "87459     4000660\n",
      "87460     4000711\n",
      "87461     4000712\n",
      "87462     4000731\n",
      "87463     4000789\n",
      "87464     4000790\n",
      "87465     4000852\n",
      "87466     4000858\n",
      "87467     4001060\n",
      "87468     4001088\n",
      "87469     4001118\n",
      "87470     4001157\n",
      "87471     4001170\n",
      "87472     4001199\n",
      "           ...   \n",
      "174698    7999009\n",
      "174699    7999032\n",
      "174700    7999040\n",
      "174701    7999059\n",
      "174702    7999065\n",
      "174703    7999081\n",
      "174704    7999105\n",
      "174705    7999139\n",
      "174706    7999171\n",
      "174707    7999183\n",
      "174708    7999213\n",
      "174709    7999275\n",
      "174710    7999277\n",
      "174711    7999281\n",
      "174712    7999309\n",
      "174713    7999343\n",
      "174714    7999370\n",
      "174715    7999420\n",
      "174716    7999430\n",
      "174717    7999462\n",
      "174718    7999542\n",
      "174719    7999566\n",
      "174720    7999601\n",
      "174721    7999650\n",
      "174722    7999679\n",
      "174723    7999696\n",
      "174724    7999775\n",
      "174725    7999808\n",
      "174726    7999971\n",
      "174727    7999987\n",
      "Name: Index, Length: 87285, dtype: int64\n",
      "         Index  label  integer_1  integer_2  integer_3  integer_4  integer_5  \\\n",
      "4000000      0    9.0          0       15.0       17.0      986.0       44.0   \n",
      "4000001      0    1.0          3        4.0        0.0       36.0        0.0   \n",
      "4000002      0    2.0         -1        NaN        NaN     1152.0       34.0   \n",
      "4000003      0    3.0         62        3.0        4.0      108.0        9.0   \n",
      "4000004      0    0.0         48        5.0        2.0     1618.0       40.0   \n",
      "\n",
      "         integer_6  integer_7  integer_8      ...        categorical_17  \\\n",
      "4000000        9.0       24.0       18.0      ...              7b06fafe   \n",
      "4000001       32.0       34.0      160.0      ...              9e4517be   \n",
      "4000002       26.0        2.0     1014.0      ...              bc5a0ff7   \n",
      "4000003        3.0        9.0        9.0      ...              74ef3502   \n",
      "4000004        4.0        0.0        5.0      ...              e8623312   \n",
      "\n",
      "         categorical_18  categorical_19  categorical_20 categorical_21  \\\n",
      "4000000        d9aa05dc        b1252a9d        968ea5b1            NaN   \n",
      "4000001        3014a4b1        5840adea        572bdde8       c9d4222a   \n",
      "4000002        5b885066        a458ea53        660513b6       ad3062eb   \n",
      "4000003             NaN             NaN        0cb5bec5       c9d4222a   \n",
      "4000004             NaN             NaN        38307227            NaN   \n",
      "\n",
      "        categorical_22 categorical_23 categorical_24 categorical_25  \\\n",
      "4000000       423fab69       882001d1       2bf691b1       7d704f34   \n",
      "4000001       32c7478e       9fa3e01a       001f3601       d9bcfc08   \n",
      "4000002       3a171ecb       9dd84531       001f3601       ae47080f   \n",
      "4000003       423fab69       9117a34a            NaN            NaN   \n",
      "4000004       32c7478e       b889075b            NaN            NaN   \n",
      "\n",
      "        categorical_26  \n",
      "4000000        4000000  \n",
      "4000001        4000001  \n",
      "4000002        4000002  \n",
      "4000003        4000003  \n",
      "4000004        4000004  \n",
      "\n",
      "[5 rows x 41 columns]\n",
      "Empty DataFrame\n",
      "Columns: [Index, label, integer_1, integer_2, integer_3, integer_4, integer_5, integer_6, integer_7, integer_8, integer_9, integer_10, integer_11, integer_12, integer_13, categorical_1, categorical_2, categorical_3, categorical_4, categorical_5, categorical_6, categorical_7, categorical_8, categorical_9, categorical_10, categorical_11, categorical_12, categorical_13, categorical_14, categorical_15, categorical_16, categorical_17, categorical_18, categorical_19, categorical_20, categorical_21, categorical_22, categorical_23, categorical_24, categorical_25, categorical_26]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]\n",
      "Empty DataFrame\n",
      "Columns: [Index, label, integer_1, integer_2, integer_3, integer_4, integer_5, integer_6, integer_7, integer_8, integer_9, integer_10, integer_11, integer_12, integer_13, categorical_1, categorical_2, categorical_3, categorical_4, categorical_5, categorical_6, categorical_7, categorical_8, categorical_9, categorical_10, categorical_11, categorical_12, categorical_13, categorical_14, categorical_15, categorical_16, categorical_17, categorical_18, categorical_19, categorical_20, categorical_21, categorical_22, categorical_23, categorical_24, categorical_25, categorical_26]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]\n",
      "Empty DataFrame\n",
      "Columns: [Index, label, integer_1, integer_2, integer_3, integer_4, integer_5, integer_6, integer_7, integer_8, integer_9, integer_10, integer_11, integer_12, integer_13, categorical_1, categorical_2, categorical_3, categorical_4, categorical_5, categorical_6, categorical_7, categorical_8, categorical_9, categorical_10, categorical_11, categorical_12, categorical_13, categorical_14, categorical_15, categorical_16, categorical_17, categorical_18, categorical_19, categorical_20, categorical_21, categorical_22, categorical_23, categorical_24, categorical_25, categorical_26]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]\n",
      "Extraction Stats: 0 percent: 0.0%\n",
      "Document Stats: 8000000 percent: 17.391304347826086%\n"
     ]
    }
   ],
   "source": [
    "def check_output_features(fs):\n",
    "    assert len(fs) < 39\n",
    "\n",
    "\n",
    "def check_output_read_data(data, target, n):\n",
    "    assert data.shape[0] == n\n",
    "    assert target.shape[0] == n\n",
    "\n",
    "\n",
    "def check_output_preprocess(preprocess):\n",
    "    assert (preprocess == 'onehot') or (preprocess == 'rate') or (preprocess == 'tfidf')\n",
    "\n",
    "\n",
    "def check_output_preprocess_int_data(data, fs):\n",
    "    n = len([f for f in fs if f < 13])\n",
    "    assert data.shape[1] == n\n",
    "\n",
    "\n",
    "def check_output_preprocess_cat_data(data, fs, preprocess):\n",
    "    pass\n",
    "\n",
    "\n",
    "def read_features(path):\n",
    "    features = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            features.append(int(line.strip()))\n",
    "    return features\n",
    "\n",
    "\n",
    "def read_preprocess(path):\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            return line.strip()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = './dac/train.txt'\n",
    "    train = 'train_ids.txt'\n",
    "    validation = 'validation_ids.txt'\n",
    "    test = 'test_ids.txt'\n",
    "    features = 'features.txt'\n",
    "    preprocess = 'preprocess.txt'\n",
    "\n",
    "    train_data, train_target, validation_data, validation_target, test_data, test_target = \\\n",
    "        read_data(data, train, validation, test)\n",
    "\n",
    "    check_output_read_data(train_data, train_target, 1000000)\n",
    "    check_output_read_data(validation_data, validation_target, 250000)\n",
    "    check_output_read_data(test_data, test_target, 750000)\n",
    "\n",
    "    features = read_features(options.features)\n",
    "\n",
    "    check_output_features(features)\n",
    "\n",
    "    preprocess = read_preprocess(options.preprocess)\n",
    "\n",
    "    check_output_preprocess(prepr3ocess)\n",
    "\n",
    "    train_int_data = preprocess_int_data(train_data, features)\n",
    "    validation_int_data = preprocess_int_data(validation_data, features)\n",
    "    test_int_data = preprocess_int_data(test_data, features)\n",
    "\n",
    "    check_output_preprocess_int_data(train_int_data, features)\n",
    "    check_output_preprocess_int_data(validation_int_data, features)\n",
    "    check_output_preprocess_int_data(test_int_data, features)\n",
    "\n",
    "    train_cat_data = preprocess_cat_data(train_data, features, preprocess)\n",
    "    validation_cat_data = preprocess_cat_data(validation_data, features, preprocess)\n",
    "    test_cat_data = preprocess_cat_data(test_data, features, preprocess)\n",
    "\n",
    "    check_output_preprocess_cat_data(train_cat_data, features, preprocess)\n",
    "    check_output_preprocess_cat_data(validation_cat_data, features, preprocess)\n",
    "    check_output_preprocess_cat_data(test_cat_data, features, preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
