{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals import joblib \n",
    "\n",
    "\n",
    "from sys import getsizeof\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional helper functions required for the code to run:\n",
    "# This function defines the column headers for the data frames\n",
    "def getColumnHeaders():\n",
    "    return pd.Series(data=['label','integer_1','integer_2','integer_3',\n",
    "                                 'integer_4','integer_5','integer_6','integer_7','integer_8','integer_9',\n",
    "                                 'integer_10','integer_11','integer_12','integer_13','categorical_1',\n",
    "                                 'categorical_2','categorical_3','categorical_4','categorical_5','categorical_6',\n",
    "                                 'categorical_7','categorical_8','categorical_9','categorical_10','categorical_11',\n",
    "                                 'categorical_12','categorical_13','categorical_14','categorical_15','categorical_16',\n",
    "                                 'categorical_17','categorical_18','categorical_19','categorical_20','categorical_21',\n",
    "                                 'categorical_22','categorical_23','categorical_24','categorical_25','categorical_26','Index'])\n",
    "\n",
    "def getDataHeaders():\n",
    "    return pd.Series(data=['integer_1','integer_2','integer_3',\n",
    "                                 'integer_4','integer_5','integer_6','integer_7','integer_8','integer_9',\n",
    "                                 'integer_10','integer_11','integer_12','integer_13','categorical_1',\n",
    "                                 'categorical_2','categorical_3','categorical_4','categorical_5','categorical_6',\n",
    "                                 'categorical_7','categorical_8','categorical_9','categorical_10','categorical_11',\n",
    "                                 'categorical_12','categorical_13','categorical_14','categorical_15','categorical_16',\n",
    "                                 'categorical_17','categorical_18','categorical_19','categorical_20','categorical_21',\n",
    "                                 'categorical_22','categorical_23','categorical_24','categorical_25','categorical_26'])\n",
    "\n",
    "# this function generateNIndecesFrom takes a range of Indeces and randomly takes n of them, returns a pandas Series object\n",
    "def generateNIndecesFrom(n, rangeOfIndeces):\n",
    "    # print(\"Generating \" + str(n) + \" indeces from range\")\n",
    "    allIndeces = random.sample(rangeOfIndeces, n)\n",
    "    allIndeces = pd.Series(data = allIndeces)\n",
    "    allIndeces = allIndeces.sort_values().reset_index().drop(['index'],axis=1)\n",
    "    allIndeces.columns = ['Index'];\n",
    "    return allIndeces\n",
    "\n",
    "# This function takes in a dataframe and computes statistics and histograms for all columns that are not 'label', 'Index' or 'Unnamed: 0'\n",
    "# It was used in part 2.2 to generate the histograms and statistics.\n",
    "# It can for example be placed at the end of the read_data method and be passed one of the datasets to compute its statistics and histograms\n",
    "def generateSummaryStatsAndHists(train1M):\n",
    "    SummaryStats = pd.DataFrame()\n",
    "    for col in train1M.columns:\n",
    "        if (col != 'label' and col != 'Index' and col != 'Unnamed: 0'):\n",
    "\n",
    "            train1M[col][train1M['label'] == 0].value_counts().plot(kind='hist',title=col, bins=100,label='0s')\n",
    "            train1M[col][train1M['label'] == 1].value_counts().plot(kind='hist',title=col, bins=100,label='1s')\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.savefig(col)\n",
    "#             plt.show()\n",
    "            plt.gcf().clear()\n",
    "            if (train1M[col].dtype != 'O'):\n",
    "                SummaryStats[col] = train1M[col].describe()   \n",
    "    # SummaryStats.head()\n",
    "    SummaryStats.to_csv('integerStats.csv')\n",
    "    return SummaryStats\n",
    "\n",
    "\n",
    "# the generateSubSet function takes in a file, a dataFrame to put the data in as well as index values which should be extracted,\n",
    "# a number of rows per itteration (this is used to not overload the memory) , total number of rows in the file, the column headers for the new dataframe\n",
    "def generateSubSet(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers):\n",
    "    totalNumIterations = int(totalNumRows/numRowsPerItteration)\n",
    "    # print(\"Number of itterations = \" + str(totalNumIterations))\n",
    "    totalNumRowsTraversed = 0\n",
    "    prevsize = 0\n",
    "    for i in range(totalNumIterations + 1):\n",
    "\n",
    "        curData = pd.read_table(file,skiprows = i * numRowsPerItteration, nrows = numRowsPerItteration,header=None)\n",
    "        curData.index = [i for i in range(i*numRowsPerItteration,i*numRowsPerItteration + curData.shape[0])]\n",
    "        totalNumRowsTraversed = totalNumRowsTraversed + curData.shape[0]\n",
    "        \n",
    "\n",
    "        curData['Index'] = curData.index\n",
    "        curData.columns = column_headers\n",
    "        \n",
    "        curIndexRange = indexValues['Index'][(indexValues['Index'] < (i*numRowsPerItteration + numRowsPerItteration)) & (indexValues['Index'] > (i*numRowsPerItteration-1))]\n",
    "        curData = curData[curData['Index'].isin(list(curIndexRange))]\n",
    "        \n",
    "        dataFrame = pd.concat([dataFrame,curData])\n",
    "        \n",
    "#         clear_output()\n",
    "#         print(\"Extraction Stats: \" + str(dataFrame.shape[0]) + \" percent: \" + str(dataFrame.shape[0] / indexValues.shape[0] * 100) + \"%\")\n",
    "#         print(\"Document Stats: \" + str(totalNumRowsTraversed) + \" percent: \" + str(totalNumRowsTraversed/totalNumRows*100) + \"%\")\n",
    "        if (dataFrame.shape[0] - prevsize) > 500000:\n",
    "            prevsize = dataFrame.shape[0]\n",
    "      \n",
    "    return dataFrame\n",
    "\n",
    "# This method generates is a wrapper around the generateSubset to generate the subset and save the dataframe to a csv file (for being able to make use of it after)\n",
    "def generateAndSaveSubset(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers,frameSaveName):\n",
    "    dataFrame = generateSubSet(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers)\n",
    "    dataFrame.to_csv(frameSaveName)\n",
    "    return dataFrame\n",
    "\n",
    "# This method generates the categorical data required to then apply one hot encoding on the entire dataset\n",
    "def generateCategoricalData(train1M):\n",
    "    #change to categorical\n",
    "    for col in train1M.columns[14:40]:\n",
    "        train1M[col] = train1M[col].astype('category')\n",
    "        \n",
    "        #get only the top 30 categories with highest count\n",
    "        averageNumber = train1M[col].value_counts().mean()\n",
    "#         print(averageNumber)\n",
    "        counts = train1M[col].value_counts()\n",
    "#        print(counts)\n",
    "        topFeatures = train1M[col].value_counts()[:30].index\n",
    "\n",
    "#         topFeatures = train1M[col].value_counts()[train1M[col].value_counts() > averageNumber].index\n",
    "#         print(topFeatures)\n",
    "        \n",
    "        # add the dummy category\n",
    "#         train1M[col].cat.add_categories(new_categories = 'Dummy',inplace = True)\n",
    "        categories = pd.Series(topFeatures)\n",
    "#         print(categories.shape)\n",
    "        categories.to_csv(str(col)+'_features.csv',header = False)\n",
    "        #save the categories for each column\n",
    "        #then we can set the categegories for each column\n",
    "        # and when we get dummies from pandas we have a one hot encoding that is consistent accross\n",
    "        # -> get_dummies() method does one hot encoding\n",
    "\n",
    "# This methods takes the training set and creates a scaler that is fit to the integer columns of the training set then saves\n",
    "# The model to file for future retrieval.\n",
    "def preProcessIntsAndSave(dataFrame,fileName):\n",
    "    dataFrame[dataFrame.columns[1:14]] = dataFrame[dataFrame.columns[1:14]].fillna(0)\n",
    "    dataFrame[dataFrame.columns[1:14]] = dataFrame[dataFrame.columns[1:14]].replace(-1,0)\n",
    "    dataFrame[dataFrame.columns[1:14]] = dataFrame[dataFrame.columns[1:14]].replace(-2,0)\n",
    "    \n",
    "    curScaler = StandardScaler()\n",
    "    curScaler.fit(dataFrame[dataFrame.columns[1:14]])\n",
    "    joblib.dump(curScaler, fileName)\n",
    "    return\n",
    "        \n",
    "def read_data(data_path, train_path, validation_path, test_path):\n",
    "\n",
    "#     print(data_path)\n",
    "#     print(train_path)\n",
    "#     print(validation_path)\n",
    "#     print(test_path)\n",
    "    \n",
    "    #get the ids\n",
    "    try:\n",
    "        trainIndeces = pd.read_csv(train_path, header = None)\n",
    "        validationIndeces = pd.read_csv(validation_path, header = None)\n",
    "        testingIndeces = pd.read_csv(test_path, header = None)\n",
    "    except:\n",
    "        print(\"There were not 1000000 data points\")\n",
    "        trainIndeces = generateNIndecesFrom(1000000,list(twoMIndeces['Index']))\n",
    "        trainIndeces.to_csv('train_ids.txt',index=False,header=False)\n",
    "\n",
    "        remainingIndeces = twoMIndeces['Index'][~twoMIndeces['Index'].isin(trainIndeces.values)]\n",
    "        validationIndeces = generateNIndecesFrom(250000,list(remainingIndeces))\n",
    "        validationIndeces.to_csv('validation_ids.txt',index=False,header=False)\n",
    "\n",
    "        testingIndeces = twoMIndeces['Index'][~(twoMIndeces['Index'].isin(trainIndeces.values) | twoMIndeces['Index'].isin(validationIndeces.values))]\n",
    "        testingIndeces = generateNIndecesFrom(750000,list(testingIndeces))\n",
    "        testingIndeces.to_csv('test_ids.txt',index=False,header=False)\n",
    "    \n",
    "    trainIndeces.columns = ['Index']\n",
    "    validationIndeces.columns = ['Index']\n",
    "    testingIndeces.columns = ['Index']\n",
    "\n",
    "    # Generate the actual data files\n",
    "    column_headers = getColumnHeaders()\n",
    "    train1M = pd.DataFrame()\n",
    "    train1M = generateSubSet(data_path,train1M,trainIndeces,4000000,46000000,column_headers)\n",
    "    \n",
    "    print(\"train1M done\")\n",
    "    \n",
    "#     return train1M\n",
    "    generateSummaryStatsAndHists(train1M)\n",
    "    generateCategoricalData(train1M)\n",
    "    preProcessIntsAndSave(train1M,'scalerPickle.pkl')\n",
    "    \n",
    "    validation250k = pd.DataFrame()\n",
    "    validation250k = generateSubSet(data_path,validation250k,validationIndeces,4000000,46000000,column_headers)\n",
    "\n",
    "    print(\"Validation done\")\n",
    "    \n",
    "    test750k = pd.DataFrame()\n",
    "    test750k = generateSubSet(data_path,test750k,testingIndeces,4000000,46000000,column_headers)\n",
    "    \n",
    "    print(\"test done\")\n",
    "    \n",
    "#     print(train1M.shape)\n",
    "#     print(validation250k.shape)\n",
    "#     print(test750k.shape)\n",
    "\n",
    "    return train1M[train1M.columns[1:40]].values, train1M['label'].values, validation250k[validation250k.columns[1:40]].values, validation250k['label'].values, test750k[test750k.columns[1:40]].values, test750k['label'].values\n",
    "\n",
    "def preprocess_int_data(data, features):\n",
    "    n = len([f for f in features if f < 13])\n",
    "    \n",
    "    dataFrame = pd.DataFrame()\n",
    "    for f in features:\n",
    "        if f < 13:\n",
    "            dataFrame = pd.concat([dataFrame, pd.DataFrame(data[:,f:f+1])],axis=1)\n",
    "\n",
    "    headers = getDataHeaders()\n",
    "    trueHeaders = []\n",
    "    for f in features:\n",
    "        if f < 13:\n",
    "            trueHeaders.append(headers[f])\n",
    "    \n",
    "    dataFrame.columns = trueHeaders\n",
    "\n",
    "    for f in features:\n",
    "        if f < 13:\n",
    "\n",
    "            dataFrame[dataFrame.columns[f]] = dataFrame[dataFrame.columns[f]].fillna(0)\n",
    "            dataFrame[dataFrame.columns[f]] = dataFrame[dataFrame.columns[f]].replace(-1,0)\n",
    "            dataFrame[dataFrame.columns[f]] = dataFrame[dataFrame.columns[f]].replace(-2,0)\n",
    "\n",
    "    scaler = joblib.load('scalerPickle.pkl') \n",
    "    scaledValues = scaler.transform(dataFrame)\n",
    "    \n",
    "    return scaledValues\n",
    "\n",
    "\n",
    "def preprocess_cat_data(data, features, preprocess):\n",
    "#     print(features)\n",
    "#     print(preprocess)\n",
    "#     print(data)\n",
    "    dataFrame = pd.DataFrame(data)\n",
    "#     print(dataFrame[dataFrame.columns[13:39]])\n",
    "    # Change each column in the 13-39 into categorical\n",
    "#     dataFrame.columns = getDataHeaders()\n",
    "    \n",
    "    returnFrame = pd.SparseDataFrame()\n",
    "    # drop the cols that are not in the features vector\n",
    "    for col in dataFrame.columns:\n",
    "        foundCol = False\n",
    "        for f in features:\n",
    "            if (f == col):\n",
    "                foundCol = True\n",
    "                \n",
    "        if foundCol == False:\n",
    "            dataFrame.drop(col,inplace=True,axis=1)\n",
    "        else:    \n",
    "            # I know that the categorical features start at 1 and index 13 so add 12 to f\n",
    "            if (col > 12):\n",
    "#                 print(col)\n",
    "#                 print(dataFrame[col].dtype)\n",
    "                dataFrame[col] = dataFrame[col].astype('category')\n",
    "                curFeatures = pd.read_csv(\"categorical_\" + str(col-12) + \"_features.csv\",header = None,index_col = 0)\n",
    "#                 print(curFeatures.values)\n",
    "                dataFrame[col].cat.set_categories(curFeatures.values,inplace = True)\n",
    "                dataFrame[col].cat.add_categories(new_categories = 'Dummy',inplace = True)\n",
    "                dataFrame[col] = dataFrame[col].fillna('Dummy')\n",
    "#                 print(dataFrame[col].dtype)\n",
    "# #                 print(dataFrame[col].cat.categories)\n",
    "                onehotVals = pd.get_dummies(dataFrame[col],prefix='encoded_'+ str(col) + \"_\",sparse=True,columns = curFeatures.values)\n",
    "#                 print(onehotVals.info())\n",
    "                returnFrame = pd.concat([returnFrame, onehotVals],axis=1)\n",
    "#                 print(\"Got 1hot for \" + str(col))\n",
    "#                 print(returnFrame.info())\n",
    "#                 return\n",
    "    \n",
    "#     print(dataFrame.head())\n",
    "#     print(dataFrame.info())\n",
    "#     print(returnFrame.head())\n",
    "    \n",
    "#     for col in dataFrame.columns[13:39]:\n",
    "#         dataFrame[col] = dataFrame[col].astype('category')\n",
    "#         #reset the categories to the ones for that column\n",
    "#         \n",
    "        \n",
    "#         dataFrame[col].cat.set_categories(curFeatures.values)\n",
    "#         pd.get_dummies(train1M[col],prefix=['encoded'],sparse=True)\n",
    "#     dataFrame[dataFrame.columns[13:39]] = dataFrame[dataFrame.columns[13:39]].fillna('Dummy')\n",
    "#     print(returnFrame.head())\n",
    "    print(returnFrame.shape)\n",
    "    print(returnFrame.columns)\n",
    "    return returnFrame.to_coo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def check_output_features(fs):\n",
    "    assert len(fs) < 39\n",
    "\n",
    "\n",
    "def check_output_read_data(data, target, n):\n",
    "    assert data.shape[0] == n\n",
    "    assert target.shape[0] == n\n",
    "\n",
    "\n",
    "def check_output_preprocess(preprocess):\n",
    "    assert (preprocess == 'onehot') or (preprocess == 'rate') or (preprocess == 'tfidf')\n",
    "\n",
    "\n",
    "def check_output_preprocess_int_data(data, fs):\n",
    "    n = len([f for f in fs if f < 13])\n",
    "    assert data.shape[1] == n\n",
    "\n",
    "\n",
    "def check_output_preprocess_cat_data(data, fs, preprocess):\n",
    "    pass\n",
    "\n",
    "\n",
    "def read_features(path):\n",
    "    features = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            features.append(int(line.strip()))\n",
    "    return features\n",
    "\n",
    "\n",
    "def read_preprocess(path):\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            return line.strip()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = './dac/train.txt'\n",
    "train = 'train_ids.txt'\n",
    "validation = 'validation_ids.txt'\n",
    "test = 'test_ids.txt'\n",
    "features = 'features.txt'\n",
    "preprocess = 'preprocess.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train1M = read_data(data, train, validation, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generateCategoricalData(train1M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train1M done\n",
      "Validation done\n",
      "test done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc64fc3b128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_data, train_target, validation_data, validation_target, test_data, test_target = \\\n",
    "    read_data(data, train, validation, test)\n",
    "    \n",
    "check_output_read_data(train_data, train_target, 1000000)\n",
    "check_output_read_data(validation_data, validation_target, 250000)\n",
    "check_output_read_data(test_data, test_target, 750000)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# # backup\n",
    "# train_data2 = train_data\n",
    "# train_target2 = train_target\n",
    "# validation_data2 = validation_data\n",
    "# validation_target2 = validation_target\n",
    "# test_data2 = test_data\n",
    "# test_target2 = test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_data2\n",
    "# del train_target2\n",
    "# del validation_data2\n",
    "# del validation_target2\n",
    "# del test_data2\n",
    "# del test_target2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_target.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(train_data, 'train_data.pkl')\n",
    "joblib.dump(train_target, 'train_target.pkl')\n",
    "joblib.dump(validation_data, 'validation_data.pkl')\n",
    "joblib.dump(validation_target, 'validation_target.pkl')\n",
    "joblib.dump(test_data, 'test_data.pkl')\n",
    "joblib.dump(test_target, 'test_target.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #restore\n",
    "# train_data = train_data2\n",
    "# train_target = train_target2\n",
    "# validation_data = validation_data2\n",
    "# validation_target = validation_target2\n",
    "# test_data = test_data2\n",
    "# test_target = test_target2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = joblib.load('train_data.pkl')\n",
    "# train_target = joblib.load('train_target.pkl')\n",
    "# validation_data = joblib.load('validation_data.pkl')\n",
    "# validation_target = joblib.load('validation_target.pkl')\n",
    "# test_data = joblib.load('test_data.pkl')\n",
    "# test_target = joblib.load('test_target.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = read_features(features)\n",
    "\n",
    "check_output_features(features)\n",
    "\n",
    "preprocess = read_preprocess(preprocess)\n",
    "\n",
    "check_output_preprocess(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_int_data = preprocess_int_data(train_data, features)\n",
    "validation_int_data = preprocess_int_data(validation_data, features)\n",
    "test_int_data = preprocess_int_data(test_data, features)\n",
    "\n",
    "check_output_preprocess_int_data(train_int_data, features)\n",
    "check_output_preprocess_int_data(validation_int_data, features)\n",
    "check_output_preprocess_int_data(test_int_data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getsizeof(train_data)+getsizeof(train_target)+getsizeof(validation_data)+getsizeof(validation_target)+getsizeof(test_data)+getsizeof(test_target)+getsizeof(train_int_data)+getsizeof(validation_int_data)+getsizeof(test_int_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 309)\n",
      "Index(['encoded_14__('38a947a1',)', 'encoded_14__('207b2d81',)',\n",
      "       'encoded_14__('38d50e09',)', 'encoded_14__('1cfdf714',)',\n",
      "       'encoded_14__('287130e0',)', 'encoded_14__('4f25e98b',)',\n",
      "       'encoded_14__('09e68b86',)', 'encoded_14__('421b43cd',)',\n",
      "       'encoded_14__('58e67aaf',)', 'encoded_14__('80e26c9b',)',\n",
      "       ...\n",
      "       'encoded_37__('3d2bedd7',)', 'encoded_37__('33d94071',)',\n",
      "       'encoded_37__('fd2fe0bd',)', 'encoded_37__('e13f3bf1',)',\n",
      "       'encoded_37__('9721386e',)', 'encoded_37__('875ea8a7',)',\n",
      "       'encoded_37__('8f8c5acd',)', 'encoded_37__('c0812fc5',)',\n",
      "       'encoded_37__('f5b6afe5',)', 'encoded_37__Dummy'],\n",
      "      dtype='object', length=309)\n",
      "(250000, 309)\n",
      "Index(['encoded_14__('38a947a1',)', 'encoded_14__('207b2d81',)',\n",
      "       'encoded_14__('38d50e09',)', 'encoded_14__('1cfdf714',)',\n",
      "       'encoded_14__('287130e0',)', 'encoded_14__('4f25e98b',)',\n",
      "       'encoded_14__('09e68b86',)', 'encoded_14__('421b43cd',)',\n",
      "       'encoded_14__('58e67aaf',)', 'encoded_14__('80e26c9b',)',\n",
      "       ...\n",
      "       'encoded_37__('3d2bedd7',)', 'encoded_37__('33d94071',)',\n",
      "       'encoded_37__('fd2fe0bd',)', 'encoded_37__('e13f3bf1',)',\n",
      "       'encoded_37__('9721386e',)', 'encoded_37__('875ea8a7',)',\n",
      "       'encoded_37__('8f8c5acd',)', 'encoded_37__('c0812fc5',)',\n",
      "       'encoded_37__('f5b6afe5',)', 'encoded_37__Dummy'],\n",
      "      dtype='object', length=309)\n",
      "(750000, 309)\n",
      "Index(['encoded_14__('38a947a1',)', 'encoded_14__('207b2d81',)',\n",
      "       'encoded_14__('38d50e09',)', 'encoded_14__('1cfdf714',)',\n",
      "       'encoded_14__('287130e0',)', 'encoded_14__('4f25e98b',)',\n",
      "       'encoded_14__('09e68b86',)', 'encoded_14__('421b43cd',)',\n",
      "       'encoded_14__('58e67aaf',)', 'encoded_14__('80e26c9b',)',\n",
      "       ...\n",
      "       'encoded_37__('3d2bedd7',)', 'encoded_37__('33d94071',)',\n",
      "       'encoded_37__('fd2fe0bd',)', 'encoded_37__('e13f3bf1',)',\n",
      "       'encoded_37__('9721386e',)', 'encoded_37__('875ea8a7',)',\n",
      "       'encoded_37__('8f8c5acd',)', 'encoded_37__('c0812fc5',)',\n",
      "       'encoded_37__('f5b6afe5',)', 'encoded_37__Dummy'],\n",
      "      dtype='object', length=309)\n"
     ]
    }
   ],
   "source": [
    "train_cat_data = preprocess_cat_data(train_data, features, preprocess)\n",
    "validation_cat_data = preprocess_cat_data(validation_data, features, preprocess)\n",
    "test_cat_data = preprocess_cat_data(test_data, features, preprocess)\n",
    "\n",
    "check_output_preprocess_cat_data(train_cat_data, features, preprocess)\n",
    "check_output_preprocess_cat_data(validation_cat_data, features, preprocess)\n",
    "check_output_preprocess_cat_data(test_cat_data, features, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 309)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 309)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_cat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750000, 309)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
