{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "from IPython.display import clear_output\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create an index file for the huge data file\n",
    "def createIndex(filePath):\n",
    "    file = open(filePath,'r');\n",
    "    indexFilePath = filePath+\"_index.csv\"\n",
    "    indexFile = open(indexFilePath,'w');\n",
    "    indexFile.close()\n",
    "    \n",
    "    offset = 0\n",
    "    lineNumber = 0\n",
    "    for line in file:\n",
    "#         clear_output(wait=True)\n",
    "#         print(lineNumber)\n",
    "        indexFile = open(indexFilePath,'a');\n",
    "        indexFile.write(str(lineNumber) + \",\" + str(offset) + \"\\n\")\n",
    "        indexFile.close()\n",
    "        lineNumber = lineNumber + 1\n",
    "        offset += len(line)\n",
    "    file.close()\n",
    "    return indexFilePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateNIndecesFrom(n, rangeOfIndeces):\n",
    "    print(\"Generating \" + str(n) + \" indeces from range\")\n",
    "    allIndeces = random.sample(rangeOfIndeces, n)\n",
    "    allIndeces = pd.Series(data = allIndeces)\n",
    "    allIndeces = allIndeces.sort_values().reset_index().drop(['index'],axis=1)\n",
    "    allIndeces.columns = ['Index'];\n",
    "    return allIndeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateAndSaveSubset(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers,frameSaveName):\n",
    "    dataFrame = generateSubSet(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers)\n",
    "    dataFrame.to_csv(frameSaveName)\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateSubSet(file,dataFrame,indexValues,numRowsPerItteration,totalNumRows,column_headers):\n",
    "    totalNumIterations = int(totalNumRows/numRowsPerItteration)\n",
    "    print(\"Number of itterations = \" + str(totalNumIterations))\n",
    "    totalNumRowsTraversed = 0\n",
    "    prevsize = 0\n",
    "    for i in range(totalNumIterations + 1):\n",
    "#         \n",
    "#         print(\"Itteration number: \" + str(i))\n",
    "#         print(\"skipRows: \" + str(i * numRowsPerItteration))\n",
    "#         print(\"Read in : \" + str(numRowsPerItteration))\n",
    "        curData = pd.read_table(file,skiprows = i * numRowsPerItteration, nrows = numRowsPerItteration,header=None)\n",
    "        curData.index = [i for i in range(i*numRowsPerItteration,i*numRowsPerItteration + curData.shape[0])]\n",
    "        totalNumRowsTraversed = totalNumRowsTraversed + curData.shape[0]\n",
    "\n",
    "#         print(curData.shape)\n",
    "#         print(curData.index.shape)\n",
    "\n",
    "        \n",
    "        curData.columns = column_headers\n",
    "        curData['Index'] = curData.index\n",
    "#         print(indexValues)\n",
    "#         break\n",
    "        curIndexRange = indexValues['Index'][(indexValues['Index'] < (i*numRowsPerItteration + numRowsPerItteration)) & (indexValues['Index'] > (i*numRowsPerItteration-1))]\n",
    "        curData = curData[curData['Index'].isin(curIndexRange)]\n",
    "\n",
    "        dataFrame = pd.concat([dataFrame,curData])\n",
    "        \n",
    "        clear_output()\n",
    "        print(\"Extraction Stats: \" + str(dataFrame.shape[0]) + \" percent: \" + str(dataFrame.shape[0] / indexValues.shape[0] * 100) + \"%\")\n",
    "        print(\"Document Stats: \" + str(totalNumRowsTraversed) + \" percent: \" + str(totalNumRowsTraversed/totalNumRows*100) + \"%\")\n",
    "        if (dataFrame.shape[0] - prevsize) > 500000:\n",
    "            prevsize = dataFrame.shape[0]\n",
    "#             dataFrame.to_csv(frameSaveName)\n",
    "        elif dataFrame.shape[0] == indexValues.shape[0]:\n",
    "            print(\"Finished with the data collection\")\n",
    "#             dataFrame.to_csv(frameSaveName)\n",
    "            break\n",
    "    print(\"Extraction is Done, now saving frame\")        \n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were not 2000000 data points\n",
      "Generating 2000000 indeces from range\n"
     ]
    }
   ],
   "source": [
    "#Load or Generate the 2M indeces to use\n",
    "try:\n",
    "    twoMIndeces = pd.read_csv('2MIndeces.csv',squeeze = True)\n",
    "    \n",
    "except:\n",
    "    print(\"There were not 2000000 data points\")\n",
    "    twoMIndeces = generateNIndecesFrom(2000000,range(0,45840617)) # this range is because there are this number of records in the training set.\n",
    "    twoMIndeces.to_csv('2MIndeces.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were not 1000000 data points\n",
      "Generating 1000000 indeces from range\n",
      "Generating 250000 indeces from range\n"
     ]
    }
   ],
   "source": [
    "#Load or Generate the 1M indeces for train, 250k validation and 750k test\n",
    "try:\n",
    "    trainIndeces = pd.read_csv('train_ids.txt',squeeze = True)\n",
    "    validationIndeces = pd.read_csv('validation_ids.txt',squeeze = True)\n",
    "except:\n",
    "    print(\"There were not 1000000 data points\")\n",
    "    trainIndeces = generateNIndecesFrom(1000000,list(twoMIndeces['Index']))\n",
    "    trainIndeces.to_csv('train_ids.txt',index=False,header=False)\n",
    "\n",
    "    remainingIndeces = twoMIndeces['Index'][~twoMIndeces['Index'].isin(trainIndeces.values)]\n",
    "    validationIndeces = generateNIndecesFrom(250000,list(remainingIndeces))\n",
    "    validationIndeces.to_csv('validation_ids.txt',index=False,header=False)\n",
    "    \n",
    "    testingIndeces = twoMIndeces['Index'][~(twoMIndeces['Index'].isin(trainIndeces.values) | twoMIndeces['Index'].isin(validationIndeces.values))]\n",
    "    testingIndeces.to_csv('testing_ids.txt',index=False,header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getColumnHeaders():\n",
    "    return pd.Series(data=['Index','label','integer_1','integer_2','integer_3',\n",
    "                                 'integer_4','integer_5','integer_6','integer_7','integer_8','integer_9'\n",
    "                                 'integer_10','integer_11','integer_12','integer_13','categorical_1',\n",
    "                                 'categorical_2','categorical_3','categorical_4','categorical_5','categorical_6',\n",
    "                                 'categorical_7','categorical_8','categorical_9','categorical_10','categorical_11',\n",
    "                                 'categorical_12','categorical_13','categorical_14','categorical_15','categorical_16',\n",
    "                                 'categorical_17','categorical_18','categorical_19','categorical_20','categorical_21',\n",
    "                                 'categorical_22','categorical_23','categorical_24','categorical_25','categorical_26'])\n",
    "column_headers = getColumnHeaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction Stats: 87499 percent: 8.7499%\n",
      "Document Stats: 4000000 percent: 8.695652173913043%\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train1M = pd.read_csv('train1M.csv',squeeze = True)\n",
    "except:\n",
    "    print(\"No 1M collection\")\n",
    "    train1M = pd.DataFrame()\n",
    "    train1M = generateAndSaveSubset('dac/train.txt',train1M,trainIndeces,4000000,46000000,column_headers,'train1M.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    validation250k = pd.read_csv('validation250k.csv',squeeze = True)\n",
    "except:\n",
    "    print(\"No 1M collection\")\n",
    "    validation250k = pd.DataFrame()\n",
    "    validation250k = generateAndSaveSubset('dac/train.txt',validation250k,validationIndeces,4000000,46000000,column_headers,'validation250k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test750k = pd.read_csv('test750k.csv',squeeze = True)\n",
    "except:\n",
    "    print(\"No 1M collection\")\n",
    "    test750k = pd.DataFrame()\n",
    "    test750k = generateAndSaveSubset('dac/train.txt',test750k,validationIndeces,4000000,46000000,column_headers,'test750k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train1M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation250k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test750k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
